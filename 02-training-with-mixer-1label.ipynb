{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a baseline trained only with PPL, which has already converged and will be used as our baseline.\n",
    "# they do teacher forcing all the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_iterator, get_dataset\n",
    "from classifiers import theme_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe \n",
    "GLOVE_EMBEDDING = GloVe(name=\"6B\", dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset, review_text_FIELD, theme_FIELD = get_dataset(vectors = GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_iter = get_iterator(train_dataset, batch_size, train=True, shuffle=True, repeat=False)\n",
    "val_iter = get_iterator(val_dataset, batch_size, train=False, shuffle=True, repeat=False)\n",
    "test_iter = get_iterator(test_dataset, batch_size, train=False, shuffle=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = list(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> awesome sauce <eos> | other\n",
      "<sos> totally recommend <eos> | other\n",
      "<sos> <unk> . <eos> | other\n",
      "<sos> truly amazing <eos> | other\n",
      "<sos> talky . <eos> | other\n",
      "<sos> must see <eos> | other\n",
      "<sos> 3.5 5 <eos> | other\n",
      "<sos> 5 stars <eos> | other\n",
      "<sos> movie ? <eos> | other\n",
      "<sos> brutal . <eos> | other\n"
     ]
    }
   ],
   "source": [
    "batch = val_list[50]\n",
    "x = batch.review_text.transpose(1, 0).int()[:10]\n",
    "y = batch.theme.int()\n",
    "\n",
    "for idx in range(x.shape[0]):\n",
    "    #print(x.shape, y.shape)\n",
    "    print(\"{} | {}\".format(' '.join([train_dataset.fields['review_text'].vocab.itos[_] for _ in x[idx]]),\n",
    "         train_dataset.fields['theme'].vocab.itos[y[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', 'other', 'plot', 'acting', 'effect', 'production']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_FIELD.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12304, 5, 300, torch.Size([12304, 300]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = review_text_FIELD.vocab.vectors.shape[0]\n",
    "label_size = len(theme_FIELD.vocab) - 1\n",
    "emb_dim = review_text_FIELD.vocab.vectors.shape[1]\n",
    "vectors = train_dataset.fields[\"review_text\"].vocab.vectors\n",
    "hidden_dim = 1024\n",
    "layers = 2\n",
    "dropout = .5\n",
    "\n",
    "vocab_size, label_size, emb_dim, vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_model import BaseModel, repackage_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_token = review_text_FIELD.vocab.stoi['<eos>']\n",
    "EOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, data, labels, i):\n",
    "    \n",
    "    split_tf = data.shape[0] - (i % data.shape[0])\n",
    "    #print(split_tf)\n",
    "    \n",
    "    seq_len = data.shape[0]\n",
    "    data_tf = data[:split_tf,:]\n",
    "    data_nf = data[split_tf:,:]\n",
    "\n",
    "    output_flat = None\n",
    "    hidden = None\n",
    "    \n",
    "    if split_tf > 0:\n",
    "\n",
    "        data = data_tf\n",
    "        output_tf, hidden = model(data, labels, hidden)\n",
    "        repackage_hidden(hidden)\n",
    "        output_flat = output_tf.contiguous().view(-1, vocab_size)\n",
    "\n",
    "    if split_tf < seq_len:\n",
    "\n",
    "        data = data_nf\n",
    "        shape = tuple((*data_nf.shape, vocab_size))\n",
    "        output_nf = torch.zeros(shape).cuda()\n",
    "        hidden_i = None\n",
    "        data_i = data[0,:]    \n",
    "\n",
    "        for di in range(data_nf.shape[0]):\n",
    "            params = data_i.unsqueeze(0), labels, hidden_i\n",
    "            output_i, hidden_i = model(*params)\n",
    "            hidden_i = repackage_hidden(hidden_i)\n",
    "            topv, topi = output_i.topk(1)\n",
    "            data_i = topi.squeeze().detach()\n",
    "            output_nf[di,:] = output_i\n",
    "\n",
    "        temp_output_flat = output_nf.contiguous().view(-1, vocab_size)\n",
    "        if output_flat is None:\n",
    "            output_flat = temp_output_flat\n",
    "        else:\n",
    "            output_flat = torch.cat([output_flat, temp_output_flat], 0)\n",
    "            \n",
    "    return output_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transtaltion2string(raw_translations, target_vocab = review_text_FIELD, max_words=30000):\n",
    "    string_translations = []\n",
    "    for raw_sentence in raw_translations:\n",
    "        string_sentence = []\n",
    "        for i, word_idx in enumerate(raw_sentence):\n",
    "            if i == max_words: break\n",
    "            word = target_vocab.vocab.itos[word_idx]\n",
    "            if word != '<sos>':\n",
    "                string_sentence.append(word)\n",
    "        string_translations.append(string_sentence)\n",
    "    return [' '.join(_) for _ in string_translations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(output_flat, original_shape, ground_truth):\n",
    "    topv, topi = output_flat.topk(1, -1)\n",
    "    topi = topi.view(*original_shape)\n",
    "    \n",
    "    predictions = []\n",
    "    for i in range(topi.shape[1]):\n",
    "        predicted_class = theme_classifier(transtaltion2string([topi[:,i]])[0])\n",
    "        idx_class = theme_FIELD.vocab.stoi[predicted_class] - 1 \n",
    "        predictions.append(idx_class)\n",
    "    predictions = torch.tensor(predictions).cuda()\n",
    "    classifier_output = (predictions == ground_truth).float()\n",
    "    acc_score = classifier_output.mean().item()\n",
    "    return classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, criterion, teacher_forcing = False):\n",
    "    model.eval()\n",
    "    total_loss_e = 0\n",
    "    total_number_of_words = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_source):\n",
    "            labels = batch.theme.cuda().long() - 1\n",
    "            batch = batch.review_text.cuda().long()\n",
    "            \n",
    "            if batch.shape[0] > 3:\n",
    "                data, targets = batch[1:-1,:], batch[2:,:]\n",
    "                target_flat = targets.contiguous().view(-1)\n",
    "                \n",
    "                tf = i if not teacher_forcing else 0\n",
    "                output_flat = forward_pass(model, data, labels, tf)\n",
    "                classifier_output = get_predicted_labels(output_flat, targets.shape, labels)\n",
    "                \n",
    "                batch_loss = criterion(output_flat, target_flat, classifier_output, targets.shape).detach().item()\n",
    "                \n",
    "                number_of_words = data.shape[0] * data.shape[1]\n",
    "                total_loss_e += batch_loss * number_of_words\n",
    "                total_number_of_words += number_of_words\n",
    "            \n",
    "    #print(total_loss_e, total_number_of_words)\n",
    "    return total_loss_e / total_number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ep0, epN, train_iter, dev_iter, optimizer, criterion, \n",
    "          max_grad_norm, model_name, best_ppl = float('inf'), teacher_forcing = False):\n",
    "    \n",
    "    best_ppl = best_ppl\n",
    "    \n",
    "    len_train_iter = len(train_iter)\n",
    "    for epoch in range(ep0, epN):\n",
    "        model.train()\n",
    "        total_loss_e = 0\n",
    "        total_number_of_words = 0 \n",
    "        \n",
    "        for i, batch in enumerate(train_iter):\n",
    "\n",
    "            labels = batch.theme.cuda().long() - 1\n",
    "            batch = batch.review_text.cuda().long()\n",
    "            hidden = None\n",
    "            \n",
    "            if batch.shape[0] > 3:\n",
    "                data, targets = batch[1:-1,:], batch[2:,:]\n",
    "                \n",
    "                tf = i if not teacher_forcing else 0\n",
    "                output_flat = forward_pass(model, data, labels, tf)\n",
    "                \n",
    "                target_flat = targets.contiguous().view(-1)\n",
    "                classifier_output = get_predicted_labels(output_flat, targets.shape, labels)\n",
    "                batch_loss = criterion(output_flat, target_flat, classifier_output, targets.shape)\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                number_of_words = data.shape[0] * data.shape[1]\n",
    "                total_loss_e += batch_loss.detach().item() * number_of_words\n",
    "                total_number_of_words += number_of_words\n",
    "            \n",
    "                \n",
    "                if i % 500 == 0:\n",
    "                    cur_loss = batch_loss.detach().item() \n",
    "                    tr_ppl_print = np.exp(cur_loss)\n",
    "                    print(\"| epoch {:3d} | batch {} / {} | train_loss {} | train_ppl {}\".format(\n",
    "                            epoch, i, len_train_iter, \n",
    "                            np.round(cur_loss, 3), np.round(tr_ppl_print, 3)))\n",
    "\n",
    "                \n",
    "                if i % 4999 == 1: #len_train_iter - 1:\n",
    "                    cur_loss = batch_loss.detach().item()\n",
    "                    tr_ppl_print = np.exp(cur_loss)\n",
    "                    gc.collect()\n",
    "                    val_loss_eval = evaluate(model, dev_iter, criterion_0, teacher_forcing)\n",
    "                    val_ppl_print = np.exp(val_loss_eval)\n",
    "                    \n",
    "                    template = \"| epoch {:3d} | batch {} / {} | train_loss {} | train_ppl {} | val_loss {} | val_ppl {}\"\n",
    "                    print(template.format(\n",
    "                            epoch, i, len_train_iter, \n",
    "                            np.round(cur_loss, 3), np.round(tr_ppl_print, 3), \n",
    "                            np.round(val_loss_eval, 3), np.round(val_ppl_print, 3)))\n",
    "\n",
    "                    if val_ppl_print < best_ppl :\n",
    "                        print('old best ppl {} new best ppl {}'.format(best_ppl, val_ppl_print))\n",
    "                        best_ppl = val_ppl_print\n",
    "                        best_model_name = '{}_{}.model'.format(model_name, best_ppl)\n",
    "                        print('save model...', best_model_name)\n",
    "                        with open(best_model_name, 'wb') as file:\n",
    "                            torch.save(model, file) \n",
    "\n",
    "                    gc.collect()\n",
    "                    model.train()\n",
    "                    \n",
    "                if i == 40000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./no-teacher-forcing/model_ppl_66.324-fin.model', 'rb') as file:\n",
    "    model = torch.load(file)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (word_embedding): Embedding(12304, 300)\n",
       "  (label_embedding): Embedding(5, 20)\n",
       "  (rnn): LSTM(320, 1024, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=1024, out_features=12304, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)#, betas=(.9999, .9999))\n",
    "\n",
    "\n",
    "def criterion(predictions, targets, labels, targets_shape):\n",
    "    \"\"\"\n",
    "    labels: [1, 0, 0, 1, ...] 1: hit, 0: miss\n",
    "    \"\"\"\n",
    "    c = nn.CrossEntropyLoss(reduction='none',\n",
    "                           ignore_index=train_dataset.fields[\"review_text\"].vocab.stoi['<pad>']).cuda()\n",
    "    out = c(predictions, targets)\n",
    "    out = (out.view(*targets_shape) * (1 - labels)).mean()\n",
    "    return out\n",
    "\n",
    "def criterion_0(predictions, targets, labels, targets_shape):\n",
    "    c = nn.CrossEntropyLoss(reduction='mean',\n",
    "                           ignore_index=train_dataset.fields[\"review_text\"].vocab.stoi['<pad>']).cuda()\n",
    "    out = c(predictions, targets)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | batch 500 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   2 | batch 1000 / 35138 | train_loss 0.298 | train_ppl 1.347\n",
      "| epoch   2 | batch 1500 / 35138 | train_loss 0.677 | train_ppl 1.969\n",
      "| epoch   2 | batch 2000 / 35138 | train_loss 0.104 | train_ppl 1.11\n",
      "| epoch   2 | batch 2500 / 35138 | train_loss 0.942 | train_ppl 2.566\n",
      "| epoch   2 | batch 3000 / 35138 | train_loss 0.234 | train_ppl 1.263\n",
      "| epoch   2 | batch 3500 / 35138 | train_loss 0.49 | train_ppl 1.632\n",
      "| epoch   2 | batch 4000 / 35138 | train_loss 0.435 | train_ppl 1.545\n",
      "| epoch   2 | batch 4500 / 35138 | train_loss 0.719 | train_ppl 2.053\n",
      "| epoch   2 | batch 5000 / 35138 | train_loss 0.588 | train_ppl 1.8\n",
      "| epoch   2 | batch 5000 / 35138 | train_loss 0.588 | train_ppl 1.8 | val_loss 4.194 | val_ppl 66.285\n",
      "old best ppl 66.324 new best ppl 66.28535720340143\n",
      "save model... no-teacher-forcing/model_ppl_66.28535720340143.model\n",
      "| epoch   2 | batch 5500 / 35138 | train_loss 0.701 | train_ppl 2.017\n",
      "| epoch   2 | batch 6000 / 35138 | train_loss 0.564 | train_ppl 1.757\n",
      "| epoch   2 | batch 6500 / 35138 | train_loss 0.929 | train_ppl 2.532\n",
      "| epoch   2 | batch 7000 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   2 | batch 7500 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   2 | batch 8000 / 35138 | train_loss 0.131 | train_ppl 1.14\n",
      "| epoch   2 | batch 8500 / 35138 | train_loss 0.898 | train_ppl 2.454\n",
      "| epoch   2 | batch 9000 / 35138 | train_loss 0.787 | train_ppl 2.197\n",
      "| epoch   2 | batch 9500 / 35138 | train_loss 0.764 | train_ppl 2.147\n",
      "| epoch   2 | batch 9999 / 35138 | train_loss 0.484 | train_ppl 1.623 | val_loss 4.193 | val_ppl 66.251\n",
      "old best ppl 66.28535720340143 new best ppl 66.25136986873851\n",
      "save model... no-teacher-forcing/model_ppl_66.25136986873851.model\n",
      "| epoch   2 | batch 10000 / 35138 | train_loss 0.867 | train_ppl 2.38\n",
      "| epoch   2 | batch 10500 / 35138 | train_loss 0.549 | train_ppl 1.731\n",
      "| epoch   2 | batch 11000 / 35138 | train_loss 0.722 | train_ppl 2.058\n",
      "| epoch   2 | batch 11500 / 35138 | train_loss 0.672 | train_ppl 1.958\n",
      "| epoch   2 | batch 12000 / 35138 | train_loss 0.491 | train_ppl 1.635\n",
      "| epoch   2 | batch 12500 / 35138 | train_loss 0.394 | train_ppl 1.483\n",
      "| epoch   2 | batch 13000 / 35138 | train_loss 0.215 | train_ppl 1.24\n",
      "| epoch   2 | batch 13500 / 35138 | train_loss 0.969 | train_ppl 2.636\n",
      "| epoch   2 | batch 14000 / 35138 | train_loss 0.563 | train_ppl 1.756\n",
      "| epoch   2 | batch 14500 / 35138 | train_loss 0.547 | train_ppl 1.728\n",
      "| epoch   2 | batch 14998 / 35138 | train_loss 0.519 | train_ppl 1.68 | val_loss 4.193 | val_ppl 66.228\n",
      "old best ppl 66.25136986873851 new best ppl 66.22793937388867\n",
      "save model... no-teacher-forcing/model_ppl_66.22793937388867.model\n",
      "| epoch   2 | batch 15000 / 35138 | train_loss 0.804 | train_ppl 2.235\n",
      "| epoch   2 | batch 15500 / 35138 | train_loss 0.816 | train_ppl 2.262\n",
      "| epoch   2 | batch 16000 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   2 | batch 16500 / 35138 | train_loss 0.544 | train_ppl 1.723\n",
      "| epoch   2 | batch 17000 / 35138 | train_loss 0.818 | train_ppl 2.267\n",
      "| epoch   2 | batch 17500 / 35138 | train_loss 0.329 | train_ppl 1.39\n",
      "| epoch   2 | batch 18000 / 35138 | train_loss 0.518 | train_ppl 1.679\n",
      "| epoch   2 | batch 18500 / 35138 | train_loss 0.704 | train_ppl 2.022\n",
      "| epoch   2 | batch 19000 / 35138 | train_loss 0.617 | train_ppl 1.854\n",
      "| epoch   2 | batch 19500 / 35138 | train_loss 0.389 | train_ppl 1.475\n",
      "| epoch   2 | batch 19997 / 35138 | train_loss 0.486 | train_ppl 1.625 | val_loss 4.193 | val_ppl 66.208\n",
      "old best ppl 66.22793937388867 new best ppl 66.20773478756188\n",
      "save model... no-teacher-forcing/model_ppl_66.20773478756188.model\n",
      "| epoch   2 | batch 20000 / 35138 | train_loss 0.45 | train_ppl 1.568\n",
      "| epoch   2 | batch 20500 / 35138 | train_loss 0.435 | train_ppl 1.544\n",
      "| epoch   2 | batch 21000 / 35138 | train_loss 0.836 | train_ppl 2.306\n",
      "| epoch   2 | batch 21500 / 35138 | train_loss 0.266 | train_ppl 1.305\n",
      "| epoch   2 | batch 22000 / 35138 | train_loss 0.873 | train_ppl 2.393\n",
      "| epoch   2 | batch 22500 / 35138 | train_loss 0.774 | train_ppl 2.168\n",
      "| epoch   2 | batch 23000 / 35138 | train_loss 1.355 | train_ppl 3.876\n",
      "| epoch   2 | batch 23500 / 35138 | train_loss 1.089 | train_ppl 2.97\n",
      "| epoch   2 | batch 24000 / 35138 | train_loss 0.488 | train_ppl 1.629\n",
      "| epoch   2 | batch 24500 / 35138 | train_loss 0.846 | train_ppl 2.33\n",
      "| epoch   2 | batch 24996 / 35138 | train_loss 0.255 | train_ppl 1.29 | val_loss 4.192 | val_ppl 66.183\n",
      "old best ppl 66.20773478756188 new best ppl 66.18297827094314\n",
      "save model... no-teacher-forcing/model_ppl_66.18297827094314.model\n",
      "| epoch   2 | batch 25000 / 35138 | train_loss 0.708 | train_ppl 2.03\n",
      "| epoch   2 | batch 25500 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   2 | batch 26000 / 35138 | train_loss 0.428 | train_ppl 1.535\n",
      "| epoch   2 | batch 26500 / 35138 | train_loss 0.438 | train_ppl 1.55\n",
      "| epoch   2 | batch 27000 / 35138 | train_loss 0.464 | train_ppl 1.59\n",
      "| epoch   2 | batch 27500 / 35138 | train_loss 0.239 | train_ppl 1.27\n",
      "| epoch   2 | batch 28000 / 35138 | train_loss 0.206 | train_ppl 1.228\n",
      "| epoch   2 | batch 28500 / 35138 | train_loss 0.618 | train_ppl 1.855\n",
      "| epoch   2 | batch 29000 / 35138 | train_loss 0.526 | train_ppl 1.692\n",
      "| epoch   2 | batch 29500 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   2 | batch 29995 / 35138 | train_loss 0.713 | train_ppl 2.04 | val_loss 4.192 | val_ppl 66.152\n",
      "old best ppl 66.18297827094314 new best ppl 66.15212842782327\n",
      "save model... no-teacher-forcing/model_ppl_66.15212842782327.model\n",
      "| epoch   2 | batch 30000 / 35138 | train_loss 0.679 | train_ppl 1.972\n",
      "| epoch   2 | batch 30500 / 35138 | train_loss 0.768 | train_ppl 2.156\n",
      "| epoch   2 | batch 31000 / 35138 | train_loss 0.442 | train_ppl 1.555\n",
      "| epoch   2 | batch 31500 / 35138 | train_loss 0.898 | train_ppl 2.455\n",
      "| epoch   2 | batch 32000 / 35138 | train_loss 1.002 | train_ppl 2.724\n",
      "| epoch   2 | batch 32500 / 35138 | train_loss 0.995 | train_ppl 2.705\n",
      "| epoch   2 | batch 33000 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   2 | batch 33500 / 35138 | train_loss 0.383 | train_ppl 1.467\n",
      "| epoch   2 | batch 34000 / 35138 | train_loss 0.917 | train_ppl 2.502\n",
      "| epoch   2 | batch 34500 / 35138 | train_loss 0.985 | train_ppl 2.679\n",
      "| epoch   2 | batch 34994 / 35138 | train_loss 0.493 | train_ppl 1.637 | val_loss 4.191 | val_ppl 66.103\n",
      "old best ppl 66.15212842782327 new best ppl 66.10337727395915\n",
      "save model... no-teacher-forcing/model_ppl_66.10337727395915.model\n",
      "| epoch   2 | batch 35000 / 35138 | train_loss 0.44 | train_ppl 1.553\n"
     ]
    }
   ],
   "source": [
    "train(model,\n",
    "      ep0 = 2,\n",
    "      epN = 3,\n",
    "      train_iter = train_iter,\n",
    "      dev_iter = val_iter,\n",
    "      optimizer = optimizer,\n",
    "      criterion = criterion,\n",
    "      max_grad_norm = 10,\n",
    "      model_name = 'no-teacher-forcing/model_ppl',\n",
    "      best_ppl = 66.324, \n",
    "      teacher_forcing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   3 | batch 500 / 35138 | train_loss 0.1 | train_ppl 1.105\n",
      "| epoch   3 | batch 1000 / 35138 | train_loss 0.181 | train_ppl 1.199\n",
      "| epoch   3 | batch 1500 / 35138 | train_loss 0.531 | train_ppl 1.7\n",
      "| epoch   3 | batch 2000 / 35138 | train_loss 0.101 | train_ppl 1.106\n",
      "| epoch   3 | batch 2500 / 35138 | train_loss 0.725 | train_ppl 2.064\n",
      "| epoch   3 | batch 3000 / 35138 | train_loss 0.384 | train_ppl 1.468\n",
      "| epoch   3 | batch 3500 / 35138 | train_loss 0.615 | train_ppl 1.849\n",
      "| epoch   3 | batch 4000 / 35138 | train_loss 0.401 | train_ppl 1.493\n",
      "| epoch   3 | batch 4500 / 35138 | train_loss 0.202 | train_ppl 1.224\n",
      "| epoch   3 | batch 5000 / 35138 | train_loss 0.873 | train_ppl 2.394\n",
      "| epoch   3 | batch 5000 / 35138 | train_loss 0.873 | train_ppl 2.394 | val_loss 4.191 | val_ppl 66.098\n",
      "old best ppl 66.324 new best ppl 66.0984728286987\n",
      "save model... no-teacher-forcing/model_ppl_66.0984728286987.model\n",
      "| epoch   3 | batch 5500 / 35138 | train_loss 0.323 | train_ppl 1.382\n",
      "| epoch   3 | batch 6000 / 35138 | train_loss 0.503 | train_ppl 1.654\n",
      "| epoch   3 | batch 6500 / 35138 | train_loss 0.643 | train_ppl 1.903\n",
      "| epoch   3 | batch 7000 / 35138 | train_loss 0.207 | train_ppl 1.229\n",
      "| epoch   3 | batch 7500 / 35138 | train_loss 0.076 | train_ppl 1.079\n",
      "| epoch   3 | batch 8000 / 35138 | train_loss 0.27 | train_ppl 1.31\n",
      "| epoch   3 | batch 8500 / 35138 | train_loss 1.018 | train_ppl 2.767\n",
      "| epoch   3 | batch 9000 / 35138 | train_loss 0.966 | train_ppl 2.627\n",
      "| epoch   3 | batch 9500 / 35138 | train_loss 0.199 | train_ppl 1.221\n",
      "| epoch   3 | batch 9999 / 35138 | train_loss 0.0 | train_ppl 1.0 | val_loss 4.191 | val_ppl 66.098\n",
      "old best ppl 66.0984728286987 new best ppl 66.09835587013453\n",
      "save model... no-teacher-forcing/model_ppl_66.09835587013453.model\n",
      "| epoch   3 | batch 10000 / 35138 | train_loss 1.066 | train_ppl 2.903\n",
      "| epoch   3 | batch 10500 / 35138 | train_loss 0.213 | train_ppl 1.238\n",
      "| epoch   3 | batch 11000 / 35138 | train_loss 0.578 | train_ppl 1.783\n",
      "| epoch   3 | batch 11500 / 35138 | train_loss 0.858 | train_ppl 2.359\n",
      "| epoch   3 | batch 12000 / 35138 | train_loss 0.34 | train_ppl 1.405\n",
      "| epoch   3 | batch 12500 / 35138 | train_loss 0.756 | train_ppl 2.129\n",
      "| epoch   3 | batch 13000 / 35138 | train_loss 0.399 | train_ppl 1.49\n",
      "| epoch   3 | batch 13500 / 35138 | train_loss 1.341 | train_ppl 3.821\n",
      "| epoch   3 | batch 14000 / 35138 | train_loss 0.894 | train_ppl 2.445\n",
      "| epoch   3 | batch 14500 / 35138 | train_loss 0.189 | train_ppl 1.208\n",
      "| epoch   3 | batch 14998 / 35138 | train_loss 0.46 | train_ppl 1.584 | val_loss 4.191 | val_ppl 66.098\n",
      "old best ppl 66.09835587013453 new best ppl 66.09761467604942\n",
      "save model... no-teacher-forcing/model_ppl_66.09761467604942.model\n",
      "| epoch   3 | batch 15000 / 35138 | train_loss 0.351 | train_ppl 1.42\n",
      "| epoch   3 | batch 15500 / 35138 | train_loss 0.512 | train_ppl 1.669\n",
      "| epoch   3 | batch 16000 / 35138 | train_loss 0.389 | train_ppl 1.476\n",
      "| epoch   3 | batch 16500 / 35138 | train_loss 0.229 | train_ppl 1.257\n",
      "| epoch   3 | batch 17000 / 35138 | train_loss 0.531 | train_ppl 1.701\n",
      "| epoch   3 | batch 17500 / 35138 | train_loss 0.657 | train_ppl 1.929\n",
      "| epoch   3 | batch 18000 / 35138 | train_loss 0.556 | train_ppl 1.744\n",
      "| epoch   3 | batch 18500 / 35138 | train_loss 0.475 | train_ppl 1.608\n",
      "| epoch   3 | batch 19000 / 35138 | train_loss 0.219 | train_ppl 1.245\n",
      "| epoch   3 | batch 19500 / 35138 | train_loss 0.174 | train_ppl 1.19\n",
      "| epoch   3 | batch 19997 / 35138 | train_loss 0.264 | train_ppl 1.302 | val_loss 4.191 | val_ppl 66.089\n",
      "old best ppl 66.09761467604942 new best ppl 66.08924447613724\n",
      "save model... no-teacher-forcing/model_ppl_66.08924447613724.model\n",
      "| epoch   3 | batch 20000 / 35138 | train_loss 0.446 | train_ppl 1.562\n",
      "| epoch   3 | batch 20500 / 35138 | train_loss 0.438 | train_ppl 1.549\n",
      "| epoch   3 | batch 21000 / 35138 | train_loss 0.943 | train_ppl 2.569\n",
      "| epoch   3 | batch 21500 / 35138 | train_loss 0.262 | train_ppl 1.3\n",
      "| epoch   3 | batch 22000 / 35138 | train_loss 0.266 | train_ppl 1.304\n",
      "| epoch   3 | batch 22500 / 35138 | train_loss 0.773 | train_ppl 2.166\n",
      "| epoch   3 | batch 23000 / 35138 | train_loss 1.09 | train_ppl 2.976\n",
      "| epoch   3 | batch 23500 / 35138 | train_loss 1.41 | train_ppl 4.094\n",
      "| epoch   3 | batch 24000 / 35138 | train_loss 0.485 | train_ppl 1.624\n",
      "| epoch   3 | batch 24500 / 35138 | train_loss 0.65 | train_ppl 1.916\n",
      "| epoch   3 | batch 24996 / 35138 | train_loss 0.258 | train_ppl 1.295 | val_loss 4.191 | val_ppl 66.067\n",
      "old best ppl 66.08924447613724 new best ppl 66.0671671168178\n",
      "save model... no-teacher-forcing/model_ppl_66.0671671168178.model\n",
      "| epoch   3 | batch 25000 / 35138 | train_loss 0.744 | train_ppl 2.105\n",
      "| epoch   3 | batch 25500 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   3 | batch 26000 / 35138 | train_loss 0.7 | train_ppl 2.014\n",
      "| epoch   3 | batch 26500 / 35138 | train_loss 0.394 | train_ppl 1.484\n",
      "| epoch   3 | batch 27000 / 35138 | train_loss 0.693 | train_ppl 2.0\n",
      "| epoch   3 | batch 27500 / 35138 | train_loss 0.479 | train_ppl 1.615\n",
      "| epoch   3 | batch 28000 / 35138 | train_loss 0.423 | train_ppl 1.527\n",
      "| epoch   3 | batch 28500 / 35138 | train_loss 0.798 | train_ppl 2.222\n",
      "| epoch   3 | batch 29000 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   3 | batch 29500 / 35138 | train_loss 0.242 | train_ppl 1.274\n",
      "| epoch   3 | batch 29995 / 35138 | train_loss 0.456 | train_ppl 1.578 | val_loss 4.19 | val_ppl 66.033\n",
      "old best ppl 66.0671671168178 new best ppl 66.03317261612973\n",
      "save model... no-teacher-forcing/model_ppl_66.03317261612973.model\n",
      "| epoch   3 | batch 30000 / 35138 | train_loss 0.441 | train_ppl 1.555\n",
      "| epoch   3 | batch 30500 / 35138 | train_loss 0.538 | train_ppl 1.713\n",
      "| epoch   3 | batch 31000 / 35138 | train_loss 0.243 | train_ppl 1.275\n",
      "| epoch   3 | batch 31500 / 35138 | train_loss 0.85 | train_ppl 2.339\n",
      "| epoch   3 | batch 32000 / 35138 | train_loss 0.783 | train_ppl 2.188\n",
      "| epoch   3 | batch 32500 / 35138 | train_loss 0.803 | train_ppl 2.232\n",
      "| epoch   3 | batch 33000 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   3 | batch 33500 / 35138 | train_loss 0.369 | train_ppl 1.447\n",
      "| epoch   3 | batch 34000 / 35138 | train_loss 0.215 | train_ppl 1.239\n",
      "| epoch   3 | batch 34500 / 35138 | train_loss 0.487 | train_ppl 1.627\n",
      "| epoch   3 | batch 34994 / 35138 | train_loss 0.279 | train_ppl 1.322 | val_loss 4.189 | val_ppl 65.978\n",
      "old best ppl 66.03317261612973 new best ppl 65.97764569608296\n",
      "save model... no-teacher-forcing/model_ppl_65.97764569608296.model\n",
      "| epoch   3 | batch 35000 / 35138 | train_loss 0.223 | train_ppl 1.25\n",
      "| epoch   4 | batch 500 / 35138 | train_loss 0.401 | train_ppl 1.494\n",
      "| epoch   4 | batch 1000 / 35138 | train_loss 0.259 | train_ppl 1.295\n",
      "| epoch   4 | batch 1500 / 35138 | train_loss 0.466 | train_ppl 1.593\n",
      "| epoch   4 | batch 2000 / 35138 | train_loss 0.246 | train_ppl 1.279\n",
      "| epoch   4 | batch 2500 / 35138 | train_loss 0.881 | train_ppl 2.412\n",
      "| epoch   4 | batch 3000 / 35138 | train_loss 0.559 | train_ppl 1.749\n",
      "| epoch   4 | batch 3500 / 35138 | train_loss 0.472 | train_ppl 1.604\n",
      "| epoch   4 | batch 4000 / 35138 | train_loss 0.469 | train_ppl 1.599\n",
      "| epoch   4 | batch 4500 / 35138 | train_loss 0.45 | train_ppl 1.568\n",
      "| epoch   4 | batch 5000 / 35138 | train_loss 1.089 | train_ppl 2.97\n",
      "| epoch   4 | batch 5000 / 35138 | train_loss 1.089 | train_ppl 2.97 | val_loss 4.19 | val_ppl 65.996\n",
      "| epoch   4 | batch 5500 / 35138 | train_loss 0.475 | train_ppl 1.608\n",
      "| epoch   4 | batch 6000 / 35138 | train_loss 0.46 | train_ppl 1.584\n",
      "| epoch   4 | batch 6500 / 35138 | train_loss 0.648 | train_ppl 1.913\n",
      "| epoch   4 | batch 7000 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   4 | batch 7500 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   4 | batch 8000 / 35138 | train_loss 0.265 | train_ppl 1.303\n",
      "| epoch   4 | batch 8500 / 35138 | train_loss 0.851 | train_ppl 2.343\n",
      "| epoch   4 | batch 9000 / 35138 | train_loss 0.963 | train_ppl 2.618\n",
      "| epoch   4 | batch 9500 / 35138 | train_loss 1.021 | train_ppl 2.775\n",
      "| epoch   4 | batch 9999 / 35138 | train_loss 0.283 | train_ppl 1.327 | val_loss 4.19 | val_ppl 66.016\n",
      "| epoch   4 | batch 10000 / 35138 | train_loss 0.625 | train_ppl 1.868\n",
      "| epoch   4 | batch 10500 / 35138 | train_loss 0.486 | train_ppl 1.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   4 | batch 11000 / 35138 | train_loss 0.813 | train_ppl 2.254\n",
      "| epoch   4 | batch 11500 / 35138 | train_loss 0.889 | train_ppl 2.432\n",
      "| epoch   4 | batch 12000 / 35138 | train_loss 0.358 | train_ppl 1.431\n",
      "| epoch   4 | batch 12500 / 35138 | train_loss 0.575 | train_ppl 1.777\n",
      "| epoch   4 | batch 13000 / 35138 | train_loss 0.0 | train_ppl 1.0\n",
      "| epoch   4 | batch 13500 / 35138 | train_loss 0.726 | train_ppl 2.066\n",
      "| epoch   4 | batch 14000 / 35138 | train_loss 0.645 | train_ppl 1.906\n",
      "| epoch   4 | batch 14500 / 35138 | train_loss 0.594 | train_ppl 1.811\n",
      "| epoch   4 | batch 14998 / 35138 | train_loss 0.696 | train_ppl 2.005 | val_loss 4.19 | val_ppl 66.029\n",
      "| epoch   4 | batch 15000 / 35138 | train_loss 0.218 | train_ppl 1.244\n",
      "| epoch   4 | batch 15500 / 35138 | train_loss 0.436 | train_ppl 1.547\n",
      "| epoch   4 | batch 16000 / 35138 | train_loss 0.133 | train_ppl 1.142\n",
      "| epoch   4 | batch 16500 / 35138 | train_loss 0.225 | train_ppl 1.252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2914a40753ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'no-teacher-forcing/model_ppl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mbest_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m66.324\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       teacher_forcing = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-4e9ac4281adf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, ep0, epN, train_iter, dev_iter, optimizer, criterion, max_grad_norm, model_name, best_ppl, teacher_forcing)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mnumber_of_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mtotal_loss_e\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumber_of_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mtotal_number_of_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnumber_of_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model,\n",
    "      ep0 = 3,\n",
    "      epN = 6,\n",
    "      train_iter = train_iter,\n",
    "      dev_iter = val_iter,\n",
    "      optimizer = optimizer,\n",
    "      criterion = criterion,\n",
    "      max_grad_norm = 10,\n",
    "      model_name = 'no-teacher-forcing/model_ppl',\n",
    "      best_ppl = 66.324, \n",
    "      teacher_forcing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./no-teacher-forcing/adam.optimizer', 'wb') as file:\n",
    "    torch.save(optimizer.state_dict(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_model import BaseModel, repackage_hidden\n",
    "with open('./no-teacher-forcing/model_ppl_65.97764569608296.model', 'rb') as file:\n",
    "    model = torch.load(file)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.170735407199645"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def criterion(predictions, targets, labels, targets_shape):\n",
    "    \"\"\"\n",
    "    labels: [1, 0, 0, 1, ...] 1: hit, 0: miss\n",
    "    \"\"\"\n",
    "    c = nn.CrossEntropyLoss(reduction='none',\n",
    "                           ignore_index=train_dataset.fields[\"review_text\"].vocab.stoi['<pad>']).cuda()\n",
    "    out = c(predictions, targets).mean()\n",
    "    return out\n",
    "evaluate(model, test_iter, criterion, teacher_forcing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.76306182092847"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.170735407199645)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "468.717386782417"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(6.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.168267680616764"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_iter, criterion_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
