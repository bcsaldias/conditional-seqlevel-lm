{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_2c_generators import get_iterator, get_dataset\n",
    "from classifiers import theme_classifier, personal_classifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.vocab import GloVe \n",
    "GLOVE_EMBEDDING = GloVe(name=\"6B\", dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, val_dataset, test_dataset, \\\n",
    "     review_text_FIELD, theme_FIELD, perspective_FIELD) = get_dataset(vectors = GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_iter = get_iterator(train_dataset, batch_size, train=True, shuffle=True, repeat=False)\n",
    "val_iter = get_iterator(val_dataset, batch_size, train=False, shuffle=True, repeat=False)\n",
    "test_iter = get_iterator(test_dataset, batch_size, train=False, shuffle=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = list(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> their scenes are so slow . <eos> | plot | False\n",
      "<sos> five stars , do n't miss <eos> | other | False\n",
      "<sos> great cast , poorly written directed <eos> | acting | False\n",
      "<sos> great dual protagonist antagonist story . <eos> | plot | False\n",
      "<sos> boring for the first two hours <eos> | other | False\n",
      "<sos> trailer was better than movie . <eos> | effect | False\n",
      "<sos> i liked the whole movie . <eos> | other | True\n",
      "<sos> touch my heart so amazing <unk> <eos> | other | True\n",
      "<sos> i say check it out . <eos> | other | True\n",
      "<sos> i give it a b . <eos> | other | True\n"
     ]
    }
   ],
   "source": [
    "batch = val_list[500]\n",
    "x = batch.review_text.transpose(1, 0).int()[:10]\n",
    "y = batch.theme.int()\n",
    "z = batch.perspective.int()\n",
    "\n",
    "for idx in range(x.shape[0]):\n",
    "    #print(x.shape, y.shape)\n",
    "    print(\"{} | {} | {}\".format(' '.join([review_text_FIELD.vocab.itos[_] for _ in x[idx]]),\n",
    "                                          theme_FIELD.vocab.itos[y[idx]],\n",
    "                                          perspective_FIELD.vocab.itos[z[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'other', 'plot', 'acting', 'effect', 'production']\n",
      "['<unk>', 'False', 'True']\n"
     ]
    }
   ],
   "source": [
    "print(theme_FIELD.vocab.itos)\n",
    "print(perspective_FIELD.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12304, 5, 2, 300, torch.Size([12304, 300]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = review_text_FIELD.vocab.vectors.shape[0]\n",
    "label_0_size = len(theme_FIELD.vocab) - 1\n",
    "label_1_size = len(perspective_FIELD.vocab) - 1\n",
    "emb_dim = review_text_FIELD.vocab.vectors.shape[1]\n",
    "vectors = train_dataset.fields[\"review_text\"].vocab.vectors\n",
    "hidden_dim = 1024\n",
    "layers = 2\n",
    "dropout = .5\n",
    "\n",
    "vocab_size, label_0_size, label_1_size, emb_dim, vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_2_model import BaseModel, repackage_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EOS_token = review_text_FIELD.vocab.stoi['<eos>']\n",
    "EOS_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(model, data, labels, i):\n",
    "    \n",
    "    split_tf = data.shape[0] - (i % data.shape[0])\n",
    "    #print(split_tf)\n",
    "    \n",
    "    seq_len = data.shape[0]\n",
    "    data_tf = data[:split_tf+int(i==0),:]\n",
    "    data_nf = data[split_tf:,:]\n",
    "\n",
    "    output_flat = None\n",
    "    hidden = None\n",
    "    \n",
    "    if split_tf > 0:\n",
    "\n",
    "        data = data_tf\n",
    "        output_tf, hidden = model(data, labels, hidden)\n",
    "        repackage_hidden(hidden)\n",
    "        output_flat = output_tf.contiguous().view(-1, vocab_size)\n",
    "\n",
    "    if split_tf < seq_len:\n",
    "\n",
    "        data = data_nf\n",
    "        shape = tuple((*data_nf.shape, vocab_size))\n",
    "        output_nf = torch.zeros(shape).cuda()\n",
    "        hidden_i = None\n",
    "        data_i = data[0,:]    \n",
    "\n",
    "        for di in range(data_nf.shape[0]):\n",
    "            params = data_i.unsqueeze(0), labels, hidden_i\n",
    "            output_i, hidden_i = model(*params)\n",
    "            hidden_i = repackage_hidden(hidden_i)\n",
    "            topv, topi = output_i.topk(1)\n",
    "            data_i = topi.squeeze().detach()\n",
    "            output_nf[di,:] = output_i\n",
    "\n",
    "        temp_output_flat = output_nf.contiguous().view(-1, vocab_size)\n",
    "        if output_flat is None:\n",
    "            output_flat = temp_output_flat\n",
    "        else:\n",
    "            output_flat = torch.cat([output_flat, temp_output_flat], 0)\n",
    "            \n",
    "    return output_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transtaltion2string(raw_translations, target_vocab = review_text_FIELD, max_words=30000):\n",
    "    string_translations = []\n",
    "    for raw_sentence in raw_translations:\n",
    "        string_sentence = []\n",
    "        for i, word_idx in enumerate(raw_sentence):\n",
    "            if i == max_words: break\n",
    "            word = target_vocab.vocab.itos[word_idx]\n",
    "            if word != '<sos>':\n",
    "                string_sentence.append(word)\n",
    "        string_translations.append(string_sentence)\n",
    "    return [' '.join(_) for _ in string_translations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_labels(output_flat, original_shape, ground_truth):\n",
    "    topv, topi = output_flat.topk(1, -1)\n",
    "    topi = topi.view(*original_shape)\n",
    "    \n",
    "    theme_predictions = []\n",
    "    perspective_predictions = []\n",
    "    for i in range(topi.shape[1]):\n",
    "        text = transtaltion2string([topi[:,i]])[0]\n",
    "        predicted_theme = theme_classifier(text)\n",
    "        predicted_perspective = personal_classifier(text)\n",
    "        \n",
    "        idx_theme = theme_FIELD.vocab.stoi[predicted_theme] - 1 \n",
    "        idx_perspective = perspective_FIELD.vocab.stoi[predicted_perspective] - 1 \n",
    "        theme_predictions.append(idx_theme)\n",
    "        perspective_predictions.append(idx_perspective)\n",
    "        \n",
    "    theme_predictions = torch.tensor(theme_predictions).cuda()\n",
    "    perspective_predictions = torch.tensor(perspective_predictions).cuda()\n",
    "    \n",
    "    classifier_theme_output = (theme_predictions == ground_truth[0]).float()\n",
    "    classifier_perspective_output = (perspective_predictions == ground_truth[1]).float()\n",
    "    #acc_score = classifier_output.mean().item()\n",
    "    \n",
    "    classifier_output = ((classifier_theme_output + classifier_perspective_output) == 2).float()\n",
    "    \n",
    "    return classifier_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, criterion, teacher_forcing = False):\n",
    "    model.eval()\n",
    "    total_loss_e = 0\n",
    "    total_number_of_words = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_source):\n",
    "            labels_0 = batch.theme.cuda().long() - 1\n",
    "            labels_1 = batch.perspective.cuda().long() - 1\n",
    "            labels = (labels_0, labels_1)\n",
    "            batch = batch.review_text.cuda().long()\n",
    "            \n",
    "            if batch.shape[0] > 3:\n",
    "                data, targets = batch[1:-1,:], batch[2:,:]\n",
    "                target_flat = targets.contiguous().view(-1)\n",
    "                \n",
    "                tf = i if not teacher_forcing else 0\n",
    "                output_flat = forward_pass(model, data, labels, tf)\n",
    "                classifier_output = get_predicted_labels(output_flat, targets.shape, labels)\n",
    "                \n",
    "                batch_loss = criterion(output_flat, target_flat, classifier_output, targets.shape).detach().item()\n",
    "                \n",
    "                number_of_words = data.shape[0] * data.shape[1]\n",
    "                total_loss_e += batch_loss * number_of_words\n",
    "                total_number_of_words += number_of_words\n",
    "            \n",
    "    #print(total_loss_e, total_number_of_words)\n",
    "    return total_loss_e / total_number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ep0, epN, train_iter, dev_iter, optimizer, criterion, \n",
    "          max_grad_norm, model_name, best_ppl = float('inf'), teacher_forcing = False):\n",
    "    \n",
    "    best_ppl = best_ppl\n",
    "    \n",
    "    len_train_iter = len(train_iter)\n",
    "    for epoch in range(ep0, epN):\n",
    "        model.train()\n",
    "        total_loss_e = 0\n",
    "        total_number_of_words = 0 \n",
    "        \n",
    "        for i, batch in enumerate(train_iter):\n",
    "\n",
    "            labels_0 = batch.theme.cuda().long() - 1\n",
    "            labels_1 = batch.perspective.cuda().long() - 1\n",
    "            labels = (labels_0, labels_1)\n",
    "            batch = batch.review_text.cuda().long()\n",
    "            hidden = None\n",
    "            \n",
    "            if batch.shape[0] > 3:\n",
    "                data, targets = batch[1:-1,:], batch[2:,:]\n",
    "                \n",
    "                tf = i if not teacher_forcing else 0\n",
    "                output_flat = forward_pass(model, data, labels, tf)\n",
    "                \n",
    "                target_flat = targets.contiguous().view(-1)\n",
    "                classifier_output = get_predicted_labels(output_flat, targets.shape, labels)\n",
    "                batch_loss = criterion(output_flat, target_flat, classifier_output, targets.shape)\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                number_of_words = data.shape[0] * data.shape[1]\n",
    "                total_loss_e += batch_loss.detach().item() * number_of_words\n",
    "                total_number_of_words += number_of_words\n",
    "            \n",
    "                \n",
    "                if i % 500 == 0:\n",
    "                    cur_loss = batch_loss.detach().item() \n",
    "                    tr_ppl_print = np.exp(cur_loss)\n",
    "                    print(\"| epoch {:3d} | batch {} / {} | train_loss {} | train_ppl {}\".format(\n",
    "                            epoch, i, len_train_iter, \n",
    "                            np.round(cur_loss, 3), np.round(tr_ppl_print, 3)))\n",
    "\n",
    "                \n",
    "                if i % 4999 == 1: #len_train_iter - 1:\n",
    "                    cur_loss = batch_loss.detach().item()\n",
    "                    tr_ppl_print = np.exp(cur_loss)\n",
    "                    gc.collect()\n",
    "                    val_loss_eval = evaluate(model, dev_iter, criterion_1, teacher_forcing)\n",
    "                    val_ppl_print = np.exp(val_loss_eval)\n",
    "                    \n",
    "                    template = \"| epoch {:3d} | batch {} / {} | train_loss {} | train_ppl {} | val_loss {} | val_ppl {}\"\n",
    "                    print(template.format(\n",
    "                            epoch, i, len_train_iter, \n",
    "                            np.round(cur_loss, 3), np.round(tr_ppl_print, 3), \n",
    "                            np.round(val_loss_eval, 3), np.round(val_ppl_print, 3)))\n",
    "\n",
    "                    if val_ppl_print < best_ppl :\n",
    "                        print('old best ppl {} new best ppl {}'.format(best_ppl, val_ppl_print))\n",
    "                        best_ppl = val_ppl_print\n",
    "                        best_model_name = '{}_{}.model'.format(model_name, best_ppl)\n",
    "                        print('save model...', best_model_name)\n",
    "                        with open(best_model_name, 'wb') as file:\n",
    "                            torch.save(model, file) \n",
    "\n",
    "                    gc.collect()\n",
    "                    model.train()\n",
    "                    \n",
    "                if i == 40000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./baseline_2c/best_model_65.49184179009306.model', 'rb') as file:\n",
    "    model = torch.load(file)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (word_embedding): Embedding(12304, 300)\n",
       "  (label_embedding): Embedding(7, 20)\n",
       "  (rnn): LSTM(340, 1024, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=1024, out_features=12304, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)#, betas=(.9999, .9999))\n",
    "\n",
    "\n",
    "def criterion(predictions, targets, labels, targets_shape):\n",
    "    \"\"\"\n",
    "    labels: [1, 0, 0, 1, ...] 1: hit, 0: miss\n",
    "    \"\"\"\n",
    "    c = nn.CrossEntropyLoss(reduction='none',\n",
    "                           ignore_index=train_dataset.fields[\"review_text\"].vocab.stoi['<pad>']).cuda()\n",
    "    out = c(predictions, targets)\n",
    "    out = (out.view(*targets_shape) * (1 - labels)).mean()\n",
    "    return out\n",
    "\n",
    "def criterion_1(predictions, targets, labels, targets_shape):\n",
    "    c = nn.CrossEntropyLoss(reduction='none',\n",
    "                           ignore_index=train_dataset.fields[\"review_text\"].vocab.stoi['<pad>']).cuda()\n",
    "    out = c(predictions, targets)\n",
    "    out = out.mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1639207230429465"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_iter, criterion_1, teacher_forcing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.32322239515557"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.1639207230429465)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | batch 500 / 35138 | train_loss 1.744 | train_ppl 5.72\n",
      "| epoch   0 | batch 1000 / 35138 | train_loss 2.247 | train_ppl 9.459\n",
      "| epoch   0 | batch 1500 / 35138 | train_loss 2.44 | train_ppl 11.478\n",
      "| epoch   0 | batch 2000 / 35138 | train_loss 3.378 | train_ppl 29.315\n",
      "| epoch   0 | batch 2500 / 35138 | train_loss 3.422 | train_ppl 30.622\n",
      "| epoch   0 | batch 3000 / 35138 | train_loss 3.328 | train_ppl 27.874\n",
      "| epoch   0 | batch 3500 / 35138 | train_loss 3.603 | train_ppl 36.72\n",
      "| epoch   0 | batch 4000 / 35138 | train_loss 3.345 | train_ppl 28.361\n",
      "| epoch   0 | batch 4500 / 35138 | train_loss 4.058 | train_ppl 57.867\n",
      "| epoch   0 | batch 5000 / 35138 | train_loss 4.052 | train_ppl 57.491\n",
      "| epoch   0 | batch 5000 / 35138 | train_loss 4.052 | train_ppl 57.491 | val_loss 4.169 | val_ppl 64.683\n",
      "old best ppl inf new best ppl 64.6825488006041\n",
      "save model... no-teacher-forcing/model_ppl_2c__64.6825488006041.model\n",
      "| epoch   0 | batch 5500 / 35138 | train_loss 3.454 | train_ppl 31.62\n",
      "| epoch   0 | batch 6000 / 35138 | train_loss 3.741 | train_ppl 42.134\n",
      "| epoch   0 | batch 6500 / 35138 | train_loss 3.588 | train_ppl 36.157\n",
      "| epoch   0 | batch 7000 / 35138 | train_loss 3.73 | train_ppl 41.662\n",
      "| epoch   0 | batch 7500 / 35138 | train_loss 3.498 | train_ppl 33.047\n",
      "| epoch   0 | batch 8000 / 35138 | train_loss 3.236 | train_ppl 25.42\n",
      "| epoch   0 | batch 8500 / 35138 | train_loss 3.687 | train_ppl 39.92\n",
      "| epoch   0 | batch 9000 / 35138 | train_loss 4.051 | train_ppl 57.427\n",
      "| epoch   0 | batch 9500 / 35138 | train_loss 3.891 | train_ppl 48.951\n",
      "| epoch   0 | batch 9999 / 35138 | train_loss 3.693 | train_ppl 40.175 | val_loss 4.171 | val_ppl 64.787\n",
      "| epoch   0 | batch 10000 / 35138 | train_loss 3.919 | train_ppl 50.364\n",
      "| epoch   0 | batch 10500 / 35138 | train_loss 4.066 | train_ppl 58.339\n",
      "| epoch   0 | batch 11000 / 35138 | train_loss 3.818 | train_ppl 45.521\n",
      "| epoch   0 | batch 11500 / 35138 | train_loss 3.457 | train_ppl 31.715\n",
      "| epoch   0 | batch 12000 / 35138 | train_loss 3.453 | train_ppl 31.604\n",
      "| epoch   0 | batch 12500 / 35138 | train_loss 3.646 | train_ppl 38.328\n",
      "| epoch   0 | batch 13000 / 35138 | train_loss 3.698 | train_ppl 40.384\n",
      "| epoch   0 | batch 13500 / 35138 | train_loss 4.114 | train_ppl 61.195\n",
      "| epoch   0 | batch 14000 / 35138 | train_loss 3.986 | train_ppl 53.823\n",
      "| epoch   0 | batch 14500 / 35138 | train_loss 4.34 | train_ppl 76.698\n",
      "| epoch   0 | batch 14998 / 35138 | train_loss 4.135 | train_ppl 62.485 | val_loss 4.17 | val_ppl 64.732\n",
      "| epoch   0 | batch 15000 / 35138 | train_loss 3.894 | train_ppl 49.114\n",
      "| epoch   0 | batch 15500 / 35138 | train_loss 4.188 | train_ppl 65.889\n",
      "| epoch   0 | batch 16000 / 35138 | train_loss 3.882 | train_ppl 48.514\n",
      "| epoch   0 | batch 16500 / 35138 | train_loss 4.114 | train_ppl 61.165\n",
      "| epoch   0 | batch 17000 / 35138 | train_loss 3.771 | train_ppl 43.434\n",
      "| epoch   0 | batch 17500 / 35138 | train_loss 4.217 | train_ppl 67.832\n",
      "| epoch   0 | batch 18000 / 35138 | train_loss 4.282 | train_ppl 72.359\n",
      "| epoch   0 | batch 18500 / 35138 | train_loss 4.176 | train_ppl 65.092\n",
      "| epoch   0 | batch 19000 / 35138 | train_loss 3.927 | train_ppl 50.735\n",
      "| epoch   0 | batch 19500 / 35138 | train_loss 4.155 | train_ppl 63.754\n",
      "| epoch   0 | batch 19997 / 35138 | train_loss 4.131 | train_ppl 62.233 | val_loss 4.165 | val_ppl 64.378\n",
      "old best ppl 64.6825488006041 new best ppl 64.37763463531724\n",
      "save model... no-teacher-forcing/model_ppl_2c__64.37763463531724.model\n",
      "| epoch   0 | batch 20000 / 35138 | train_loss 4.417 | train_ppl 82.88\n",
      "| epoch   0 | batch 20500 / 35138 | train_loss 4.138 | train_ppl 62.693\n",
      "| epoch   0 | batch 21000 / 35138 | train_loss 4.17 | train_ppl 64.716\n",
      "| epoch   0 | batch 21500 / 35138 | train_loss 4.342 | train_ppl 76.846\n",
      "| epoch   0 | batch 22000 / 35138 | train_loss 4.629 | train_ppl 102.446\n",
      "| epoch   0 | batch 22500 / 35138 | train_loss 4.012 | train_ppl 55.263\n",
      "| epoch   0 | batch 23000 / 35138 | train_loss 4.262 | train_ppl 70.983\n",
      "| epoch   0 | batch 23500 / 35138 | train_loss 4.335 | train_ppl 76.358\n",
      "| epoch   0 | batch 24000 / 35138 | train_loss 4.673 | train_ppl 107.004\n",
      "| epoch   0 | batch 24500 / 35138 | train_loss 4.307 | train_ppl 74.2\n",
      "| epoch   0 | batch 24996 / 35138 | train_loss 4.415 | train_ppl 82.713 | val_loss 4.157 | val_ppl 63.855\n",
      "old best ppl 64.37763463531724 new best ppl 63.85457801030582\n",
      "save model... no-teacher-forcing/model_ppl_2c__63.85457801030582.model\n",
      "| epoch   0 | batch 25000 / 35138 | train_loss 4.574 | train_ppl 96.904\n",
      "| epoch   0 | batch 25500 / 35138 | train_loss 4.274 | train_ppl 71.825\n",
      "| epoch   0 | batch 26000 / 35138 | train_loss 4.179 | train_ppl 65.323\n",
      "| epoch   0 | batch 26500 / 35138 | train_loss 4.313 | train_ppl 74.652\n",
      "| epoch   0 | batch 27000 / 35138 | train_loss 4.589 | train_ppl 98.44\n",
      "| epoch   0 | batch 27500 / 35138 | train_loss 4.537 | train_ppl 93.449\n",
      "| epoch   0 | batch 28000 / 35138 | train_loss 4.281 | train_ppl 72.307\n",
      "| epoch   0 | batch 28500 / 35138 | train_loss 4.131 | train_ppl 62.218\n",
      "| epoch   0 | batch 29000 / 35138 | train_loss 4.677 | train_ppl 107.474\n",
      "| epoch   0 | batch 29500 / 35138 | train_loss 4.404 | train_ppl 81.802\n",
      "| epoch   0 | batch 29995 / 35138 | train_loss 4.633 | train_ppl 102.778 | val_loss 4.148 | val_ppl 63.281\n",
      "old best ppl 63.85457801030582 new best ppl 63.28070577427732\n",
      "save model... no-teacher-forcing/model_ppl_2c__63.28070577427732.model\n",
      "| epoch   0 | batch 30000 / 35138 | train_loss 4.555 | train_ppl 95.135\n",
      "| epoch   0 | batch 30500 / 35138 | train_loss 4.809 | train_ppl 122.549\n",
      "| epoch   0 | batch 31000 / 35138 | train_loss 4.787 | train_ppl 119.956\n",
      "| epoch   0 | batch 31500 / 35138 | train_loss 4.386 | train_ppl 80.303\n",
      "| epoch   0 | batch 32000 / 35138 | train_loss 4.655 | train_ppl 105.125\n",
      "| epoch   0 | batch 32500 / 35138 | train_loss 4.874 | train_ppl 130.868\n",
      "| epoch   0 | batch 33000 / 35138 | train_loss 4.928 | train_ppl 138.127\n",
      "| epoch   0 | batch 33500 / 35138 | train_loss 4.573 | train_ppl 96.82\n",
      "| epoch   0 | batch 34000 / 35138 | train_loss 4.686 | train_ppl 108.438\n",
      "| epoch   0 | batch 34500 / 35138 | train_loss 4.491 | train_ppl 89.175\n",
      "| epoch   0 | batch 34994 / 35138 | train_loss 5.168 | train_ppl 175.65 | val_loss 4.136 | val_ppl 62.563\n",
      "old best ppl 63.28070577427732 new best ppl 62.562653420268916\n",
      "save model... no-teacher-forcing/model_ppl_2c__62.562653420268916.model\n",
      "| epoch   0 | batch 35000 / 35138 | train_loss 4.944 | train_ppl 140.376\n"
     ]
    }
   ],
   "source": [
    "train(model,\n",
    "      ep0 = 0,\n",
    "      epN = 1,\n",
    "      train_iter = train_iter,\n",
    "      dev_iter = val_iter,\n",
    "      optimizer = optimizer,\n",
    "      criterion = criterion,\n",
    "      max_grad_norm = 10,\n",
    "      model_name = 'no-teacher-forcing/model_ppl_2c_',\n",
    "      best_ppl = float('inf'),\n",
    "      teacher_forcing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | batch 500 / 35138 | train_loss 1.625 | train_ppl 5.078\n",
      "| epoch   1 | batch 1000 / 35138 | train_loss 2.227 | train_ppl 9.274\n",
      "| epoch   1 | batch 1500 / 35138 | train_loss 2.305 | train_ppl 10.029\n",
      "| epoch   1 | batch 2000 / 35138 | train_loss 3.176 | train_ppl 23.939\n",
      "| epoch   1 | batch 2500 / 35138 | train_loss 3.326 | train_ppl 27.825\n",
      "| epoch   1 | batch 3000 / 35138 | train_loss 3.421 | train_ppl 30.596\n",
      "| epoch   1 | batch 3500 / 35138 | train_loss 3.611 | train_ppl 37.007\n",
      "| epoch   1 | batch 4000 / 35138 | train_loss 3.2 | train_ppl 24.531\n",
      "| epoch   1 | batch 4500 / 35138 | train_loss 3.981 | train_ppl 53.547\n",
      "| epoch   1 | batch 5000 / 35138 | train_loss 3.901 | train_ppl 49.449\n",
      "| epoch   1 | batch 5000 / 35138 | train_loss 3.901 | train_ppl 49.449 | val_loss 4.138 | val_ppl 62.665\n",
      "old best ppl inf new best ppl 62.664970197613926\n",
      "save model... no-teacher-forcing/model_ppl_2c__62.664970197613926.model\n",
      "| epoch   1 | batch 5500 / 35138 | train_loss 3.462 | train_ppl 31.881\n",
      "| epoch   1 | batch 6000 / 35138 | train_loss 3.674 | train_ppl 39.416\n",
      "| epoch   1 | batch 6500 / 35138 | train_loss 3.606 | train_ppl 36.804\n",
      "| epoch   1 | batch 7000 / 35138 | train_loss 3.681 | train_ppl 39.668\n",
      "| epoch   1 | batch 7500 / 35138 | train_loss 3.452 | train_ppl 31.554\n",
      "| epoch   1 | batch 8000 / 35138 | train_loss 3.116 | train_ppl 22.555\n",
      "| epoch   1 | batch 8500 / 35138 | train_loss 3.652 | train_ppl 38.566\n",
      "| epoch   1 | batch 9000 / 35138 | train_loss 4.163 | train_ppl 64.278\n",
      "| epoch   1 | batch 9500 / 35138 | train_loss 3.99 | train_ppl 54.049\n",
      "| epoch   1 | batch 9999 / 35138 | train_loss 3.697 | train_ppl 40.34 | val_loss 4.141 | val_ppl 62.893\n",
      "| epoch   1 | batch 10000 / 35138 | train_loss 3.885 | train_ppl 48.658\n",
      "| epoch   1 | batch 10500 / 35138 | train_loss 4.154 | train_ppl 63.699\n",
      "| epoch   1 | batch 11000 / 35138 | train_loss 3.803 | train_ppl 44.848\n",
      "| epoch   1 | batch 11500 / 35138 | train_loss 3.407 | train_ppl 30.179\n",
      "| epoch   1 | batch 12000 / 35138 | train_loss 3.404 | train_ppl 30.096\n",
      "| epoch   1 | batch 12500 / 35138 | train_loss 3.607 | train_ppl 36.849\n",
      "| epoch   1 | batch 13000 / 35138 | train_loss 3.681 | train_ppl 39.685\n",
      "| epoch   1 | batch 13500 / 35138 | train_loss 4.128 | train_ppl 62.061\n",
      "| epoch   1 | batch 14000 / 35138 | train_loss 3.966 | train_ppl 52.797\n",
      "| epoch   1 | batch 14500 / 35138 | train_loss 4.236 | train_ppl 69.158\n",
      "| epoch   1 | batch 14998 / 35138 | train_loss 4.161 | train_ppl 64.157 | val_loss 4.143 | val_ppl 62.992\n",
      "| epoch   1 | batch 15000 / 35138 | train_loss 3.906 | train_ppl 49.716\n",
      "| epoch   1 | batch 15500 / 35138 | train_loss 4.172 | train_ppl 64.86\n",
      "| epoch   1 | batch 16000 / 35138 | train_loss 3.908 | train_ppl 49.81\n",
      "| epoch   1 | batch 16500 / 35138 | train_loss 4.15 | train_ppl 63.449\n",
      "| epoch   1 | batch 17000 / 35138 | train_loss 3.728 | train_ppl 41.609\n",
      "| epoch   1 | batch 17500 / 35138 | train_loss 4.245 | train_ppl 69.772\n",
      "| epoch   1 | batch 18000 / 35138 | train_loss 4.133 | train_ppl 62.356\n",
      "| epoch   1 | batch 18500 / 35138 | train_loss 4.153 | train_ppl 63.596\n",
      "| epoch   1 | batch 19000 / 35138 | train_loss 3.883 | train_ppl 48.593\n",
      "| epoch   1 | batch 19500 / 35138 | train_loss 4.16 | train_ppl 64.048\n",
      "| epoch   1 | batch 19997 / 35138 | train_loss 4.087 | train_ppl 59.539 | val_loss 4.14 | val_ppl 62.823\n",
      "| epoch   1 | batch 20000 / 35138 | train_loss 4.31 | train_ppl 74.435\n",
      "| epoch   1 | batch 20500 / 35138 | train_loss 4.092 | train_ppl 59.871\n",
      "| epoch   1 | batch 21000 / 35138 | train_loss 4.165 | train_ppl 64.374\n",
      "| epoch   1 | batch 21500 / 35138 | train_loss 4.412 | train_ppl 82.452\n",
      "| epoch   1 | batch 22000 / 35138 | train_loss 4.663 | train_ppl 105.918\n",
      "| epoch   1 | batch 22500 / 35138 | train_loss 4.037 | train_ppl 56.674\n",
      "| epoch   1 | batch 23000 / 35138 | train_loss 4.238 | train_ppl 69.29\n",
      "| epoch   1 | batch 23500 / 35138 | train_loss 4.376 | train_ppl 79.483\n",
      "| epoch   1 | batch 24000 / 35138 | train_loss 4.609 | train_ppl 100.403\n",
      "| epoch   1 | batch 24500 / 35138 | train_loss 4.326 | train_ppl 75.611\n",
      "| epoch   1 | batch 24996 / 35138 | train_loss 4.393 | train_ppl 80.921 | val_loss 4.135 | val_ppl 62.481\n",
      "old best ppl 62.664970197613926 new best ppl 62.481104277979725\n",
      "save model... no-teacher-forcing/model_ppl_2c__62.481104277979725.model\n",
      "| epoch   1 | batch 25000 / 35138 | train_loss 4.599 | train_ppl 99.378\n",
      "| epoch   1 | batch 25500 / 35138 | train_loss 4.345 | train_ppl 77.091\n",
      "| epoch   1 | batch 26000 / 35138 | train_loss 4.204 | train_ppl 66.929\n",
      "| epoch   1 | batch 26500 / 35138 | train_loss 4.229 | train_ppl 68.621\n",
      "| epoch   1 | batch 27000 / 35138 | train_loss 4.55 | train_ppl 94.636\n",
      "| epoch   1 | batch 27500 / 35138 | train_loss 4.538 | train_ppl 93.515\n",
      "| epoch   1 | batch 28000 / 35138 | train_loss 4.279 | train_ppl 72.168\n",
      "| epoch   1 | batch 28500 / 35138 | train_loss 4.166 | train_ppl 64.488\n",
      "| epoch   1 | batch 29000 / 35138 | train_loss 4.621 | train_ppl 101.548\n",
      "| epoch   1 | batch 29500 / 35138 | train_loss 4.35 | train_ppl 77.475\n",
      "| epoch   1 | batch 29995 / 35138 | train_loss 4.618 | train_ppl 101.282 | val_loss 4.128 | val_ppl 62.083\n",
      "old best ppl 62.481104277979725 new best ppl 62.082719926352354\n",
      "save model... no-teacher-forcing/model_ppl_2c__62.082719926352354.model\n",
      "| epoch   1 | batch 30000 / 35138 | train_loss 4.512 | train_ppl 91.139\n",
      "| epoch   1 | batch 30500 / 35138 | train_loss 4.756 | train_ppl 116.248\n",
      "| epoch   1 | batch 31000 / 35138 | train_loss 4.758 | train_ppl 116.464\n",
      "| epoch   1 | batch 31500 / 35138 | train_loss 4.322 | train_ppl 75.305\n",
      "| epoch   1 | batch 32000 / 35138 | train_loss 4.627 | train_ppl 102.22\n",
      "| epoch   1 | batch 32500 / 35138 | train_loss 4.801 | train_ppl 121.621\n",
      "| epoch   1 | batch 33000 / 35138 | train_loss 4.891 | train_ppl 133.033\n",
      "| epoch   1 | batch 33500 / 35138 | train_loss 4.56 | train_ppl 95.629\n",
      "| epoch   1 | batch 34000 / 35138 | train_loss 4.686 | train_ppl 108.449\n",
      "| epoch   1 | batch 34500 / 35138 | train_loss 4.467 | train_ppl 87.073\n",
      "| epoch   1 | batch 34994 / 35138 | train_loss 5.126 | train_ppl 168.333 | val_loss 4.12 | val_ppl 61.585\n",
      "old best ppl 62.082719926352354 new best ppl 61.58480131769885\n",
      "save model... no-teacher-forcing/model_ppl_2c__61.58480131769885.model\n",
      "| epoch   1 | batch 35000 / 35138 | train_loss 4.906 | train_ppl 135.079\n"
     ]
    }
   ],
   "source": [
    "train(model,\n",
    "      ep0 = 1,\n",
    "      epN = 2,\n",
    "      train_iter = train_iter,\n",
    "      dev_iter = val_iter,\n",
    "      optimizer = optimizer,\n",
    "      criterion = criterion,\n",
    "      max_grad_norm = 10,\n",
    "      model_name = 'no-teacher-forcing/model_ppl_2c_',\n",
    "      best_ppl = float('inf'),\n",
    "      teacher_forcing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   2 | batch 500 / 35138 | train_loss 1.541 | train_ppl 4.671\n",
      "| epoch   2 | batch 1000 / 35138 | train_loss 2.224 | train_ppl 9.248\n",
      "| epoch   2 | batch 1500 / 35138 | train_loss 2.335 | train_ppl 10.326\n",
      "| epoch   2 | batch 2000 / 35138 | train_loss 3.281 | train_ppl 26.596\n",
      "| epoch   2 | batch 2500 / 35138 | train_loss 3.462 | train_ppl 31.878\n",
      "| epoch   2 | batch 3000 / 35138 | train_loss 3.373 | train_ppl 29.157\n",
      "| epoch   2 | batch 3500 / 35138 | train_loss 3.611 | train_ppl 37.021\n",
      "| epoch   2 | batch 4000 / 35138 | train_loss 3.279 | train_ppl 26.552\n",
      "| epoch   2 | batch 4500 / 35138 | train_loss 3.98 | train_ppl 53.498\n",
      "| epoch   2 | batch 5000 / 35138 | train_loss 3.987 | train_ppl 53.882\n",
      "| epoch   2 | batch 5000 / 35138 | train_loss 3.987 | train_ppl 53.882 | val_loss 4.122 | val_ppl 61.707\n",
      "old best ppl inf new best ppl 61.70748352585333\n",
      "save model... no-teacher-forcing/model_ppl_2c__61.70748352585333.model\n",
      "| epoch   2 | batch 5500 / 35138 | train_loss 3.362 | train_ppl 28.855\n",
      "| epoch   2 | batch 6000 / 35138 | train_loss 3.671 | train_ppl 39.285\n",
      "| epoch   2 | batch 6500 / 35138 | train_loss 3.541 | train_ppl 34.51\n",
      "| epoch   2 | batch 7000 / 35138 | train_loss 3.653 | train_ppl 38.594\n",
      "| epoch   2 | batch 7500 / 35138 | train_loss 3.462 | train_ppl 31.873\n",
      "| epoch   2 | batch 8000 / 35138 | train_loss 3.148 | train_ppl 23.294\n",
      "| epoch   2 | batch 8500 / 35138 | train_loss 3.66 | train_ppl 38.868\n",
      "| epoch   2 | batch 9000 / 35138 | train_loss 4.028 | train_ppl 56.168\n",
      "| epoch   2 | batch 9500 / 35138 | train_loss 3.997 | train_ppl 54.451\n",
      "| epoch   2 | batch 9999 / 35138 | train_loss 3.627 | train_ppl 37.612 | val_loss 4.127 | val_ppl 61.969\n",
      "| epoch   2 | batch 10000 / 35138 | train_loss 3.883 | train_ppl 48.582\n",
      "| epoch   2 | batch 10500 / 35138 | train_loss 3.943 | train_ppl 51.577\n",
      "| epoch   2 | batch 11000 / 35138 | train_loss 3.75 | train_ppl 42.504\n",
      "| epoch   2 | batch 11500 / 35138 | train_loss 3.47 | train_ppl 32.13\n",
      "| epoch   2 | batch 12000 / 35138 | train_loss 3.393 | train_ppl 29.762\n",
      "| epoch   2 | batch 12500 / 35138 | train_loss 3.634 | train_ppl 37.846\n",
      "| epoch   2 | batch 13000 / 35138 | train_loss 3.651 | train_ppl 38.513\n",
      "| epoch   2 | batch 13500 / 35138 | train_loss 4.202 | train_ppl 66.835\n",
      "| epoch   2 | batch 14000 / 35138 | train_loss 3.969 | train_ppl 52.915\n",
      "| epoch   2 | batch 14500 / 35138 | train_loss 4.305 | train_ppl 74.077\n",
      "| epoch   2 | batch 14998 / 35138 | train_loss 4.211 | train_ppl 67.416 | val_loss 4.129 | val_ppl 62.111\n",
      "| epoch   2 | batch 15000 / 35138 | train_loss 3.887 | train_ppl 48.766\n",
      "| epoch   2 | batch 15500 / 35138 | train_loss 4.167 | train_ppl 64.496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a03e4bbb1617>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'no-teacher-forcing/model_ppl_2c_'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mbest_ppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       teacher_forcing = True)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-84fa52681dd1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, ep0, epN, train_iter, dev_iter, optimizer, criterion, max_grad_norm, model_name, best_ppl, teacher_forcing)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mnumber_of_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0mtotal_loss_e\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumber_of_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mtotal_number_of_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnumber_of_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model,\n",
    "      ep0 = 2,\n",
    "      epN = 4,\n",
    "      train_iter = train_iter,\n",
    "      dev_iter = val_iter,\n",
    "      optimizer = optimizer,\n",
    "      criterion = criterion,\n",
    "      max_grad_norm = 10,\n",
    "      model_name = 'no-teacher-forcing/model_ppl_2c_',\n",
    "      best_ppl = float('inf'),\n",
    "      teacher_forcing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model,\n",
    "      ep0 = 4,\n",
    "      epN = 6,\n",
    "      train_iter = train_iter,\n",
    "      dev_iter = val_iter,\n",
    "      optimizer = optimizer,\n",
    "      criterion = criterion,\n",
    "      max_grad_norm = 10,\n",
    "      model_name = 'no-teacher-forcing/model_ppl_2c_',\n",
    "      best_ppl = float('inf'),\n",
    "      teacher_forcing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_model import BaseModel, repackage_hidden\n",
    "with open('./no-teacher-forcing/model_ppl_2c__61.58480131769885.model', 'rb') as file:\n",
    "    model = torch.load(file)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.115639458389707"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def criterion(predictions, targets, labels, targets_shape):\n",
    "    \"\"\"\n",
    "    labels: [1, 0, 0, 1, ...] 1: hit, 0: miss\n",
    "    \"\"\"\n",
    "    c = nn.CrossEntropyLoss(reduction='none',\n",
    "                           ignore_index=train_dataset.fields[\"review_text\"].vocab.stoi['<pad>']).cuda()\n",
    "    out = c(predictions, targets).mean()\n",
    "    return out\n",
    "evaluate(model, test_iter, criterion, teacher_forcing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.291395030950795"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.115639458389707)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
