{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_iterator, get_dataset\n",
    "from classifiers import theme_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe \n",
    "GLOVE_EMBEDDING = GloVe(name=\"6B\", dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset, review_text_FIELD, theme_FIELD = get_dataset(vectors = \n",
    "                                                                                       GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = get_iterator(train_dataset, 32, train=True, shuffle=False, repeat=False)\n",
    "val_iter = get_iterator(val_dataset, 32, train=False, shuffle=False, repeat=False)\n",
    "test_iter = get_iterator(test_dataset, 32, train=False, shuffle=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train = list(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great micro budget film with decent writing and acting . | plot\n",
      "vince vaughn 's acting was really good as well . | acting\n",
      "mark hamil returning as joker saves this mildly disappointing film | other\n",
      "for those in doubt , locke is the proof . | other\n",
      "great mixture of epic battles and individual character development . | plot\n",
      "i would give it 0 stars if i could . | other\n",
      "i could n't take my eyes off the screen . | other\n",
      "a slightly odd cast in the very strange film . | acting\n",
      "the overall feel is boredom interspersed with nasty violence . | other\n",
      "too many boobs and too many repeated voice overs . | effect\n",
      "and daniel radcliffe and zoe <unk> prove just that . | other\n",
      "the story seemed to be one big loop hole . | plot\n",
      "the trailer was more suspenseful than the actual movie . | effect\n",
      "i 'd give it 0.5 stars if i could . | other\n",
      "the visuals are tremendous and the acting very good . | acting\n",
      "now , that 's how you make an entrance . | other\n",
      "no plot , no acting and no reason to see | plot\n",
      "there are a lot of hurting people out there . | other\n",
      "paul walker 's emotional acting was was truly great . | acting\n",
      "great acting performances by maggie smith & tom courtenay . | acting\n",
      "it 's beautifully shot with a great creepy vibe . | effect\n",
      "gripping tale of brotherhood , tom hardy is simply amazing | plot\n",
      "her motherly instincts really bring out her acting capabilities . | acting\n",
      "that was important to note because of what follows . | other\n",
      "very good thriller in the style of primal fear . | other\n",
      "disappointing ending but i enjoyed where it was going . | other\n",
      "i 'd even rather watch the blair witch again . | other\n",
      "plot not as good as first , but great comedy | plot\n",
      "well , is this not enough more colorful people ? | other\n",
      "think twice about the water you drink next time . | other\n",
      "he went to the super villain 's base unprepared . | other\n",
      "uses some perfect scenery and extensively detailed production design . | production\n"
     ]
    }
   ],
   "source": [
    "batch = list_train[5600]\n",
    "x = batch.review_text.transpose(1, 0).int()\n",
    "y = batch.theme.int()\n",
    "\n",
    "for idx in range(x.shape[0]):\n",
    "    #print(x.shape, y.shape)\n",
    "    print(\"{} | {}\".format(' '.join([train_dataset.fields['review_text'].vocab.itos[_] for _ in x[idx]]),\n",
    "         train_dataset.fields['theme'].vocab.itos[y[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30002, 300]), 30002)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text_FIELD.vocab.vectors.shape, len(review_text_FIELD.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = review_text_FIELD.vocab.vectors.shape[0]\n",
    "emb_dim = review_text_FIELD.vocab.vectors.shape[1]\n",
    "hidden_dim = 500\n",
    "layers = 2\n",
    "dropout = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, nhid = hidden_dim, emb_dim = emb_dim, ninp = hidden_dim, \n",
    "                 nlayers = layers, ntoken = vocab_size, dropout = dropout, pretrained = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        self.pretrained = pretrained\n",
    "        if pretrained:\n",
    "            self.encoder = nn.Embedding(ntoken, emb_dim)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.rnn.flatten_parameters()\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        \n",
    "        if pretrained:\n",
    "            self.encoder.weight.data = train_dataset.fields[\"review_text\"].vocab.vectors\n",
    "            \n",
    "    def init_weights(self):\n",
    "        initrange = .1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.encoder(input)\n",
    "        emb = self.drop(emb)\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        output = output.view(output.size(0)*output.size(1), output.size(2))\n",
    "        decoded = self.decoder(output)\n",
    "        log_probs = self.softmax(decoded)\n",
    "        return log_probs, hidden\n",
    "      \n",
    "      \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        output = (weight.new_zeros(self.nlayers, bsz, self.nhid).cuda(),\n",
    "            weight.new_zeros(self.nlayers, bsz, self.nhid).cuda())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
