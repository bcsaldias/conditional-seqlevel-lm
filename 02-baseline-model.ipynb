{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_2c_generators import get_iterator, get_dataset\n",
    "from classifiers import theme_classifier, personal_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe \n",
    "GLOVE_EMBEDDING = GloVe(name=\"6B\", dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, val_dataset, test_dataset, \\\n",
    " review_text_FIELD, theme_FIELD, perspective_FIELD) = get_dataset(vectors = GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "train_iter = get_iterator(train_dataset, batch_size, train=True, shuffle=True, repeat=False)\n",
    "val_iter = get_iterator(val_dataset, batch_size, train=False, shuffle=True, repeat=False)\n",
    "test_iter = get_iterator(test_dataset, batch_size, train=False, shuffle=True, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = list(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> their scenes are so slow . <eos> | plot | False\n",
      "<sos> five stars , do n't miss <eos> | other | False\n",
      "<sos> great cast , poorly written directed <eos> | acting | False\n",
      "<sos> great dual protagonist antagonist story . <eos> | plot | False\n",
      "<sos> boring for the first two hours <eos> | other | False\n",
      "<sos> trailer was better than movie . <eos> | effect | False\n",
      "<sos> i liked the whole movie . <eos> | other | True\n",
      "<sos> touch my heart so amazing <unk> <eos> | other | True\n",
      "<sos> i say check it out . <eos> | other | True\n",
      "<sos> i give it a b . <eos> | other | True\n"
     ]
    }
   ],
   "source": [
    "batch = val_list[500]\n",
    "x = batch.review_text.transpose(1, 0).int()[:10]\n",
    "y = batch.theme.int()\n",
    "z = batch.perspective.int()\n",
    "\n",
    "for idx in range(x.shape[0]):\n",
    "    #print(x.shape, y.shape)\n",
    "    print(\"{} | {} | {}\".format(' '.join([review_text_FIELD.vocab.itos[_] for _ in x[idx]]),\n",
    "                                          theme_FIELD.vocab.itos[y[idx]],\n",
    "                                          perspective_FIELD.vocab.itos[z[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_text_FIELD.vocab.vectors.shape, len(review_text_FIELD.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'other', 'plot', 'acting', 'effect', 'production']\n",
      "['<unk>', 'False', 'True']\n"
     ]
    }
   ],
   "source": [
    "print(theme_FIELD.vocab.itos)\n",
    "print(perspective_FIELD.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12304, 5, 2, 300, torch.Size([12304, 300]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = review_text_FIELD.vocab.vectors.shape[0]\n",
    "label_0_size = len(theme_FIELD.vocab) - 1\n",
    "label_1_size = len(perspective_FIELD.vocab) - 1\n",
    "emb_dim = review_text_FIELD.vocab.vectors.shape[1]\n",
    "vectors = train_dataset.fields[\"review_text\"].vocab.vectors\n",
    "hidden_dim = 1024\n",
    "layers = 2\n",
    "dropout = .5\n",
    "\n",
    "vocab_size, label_0_size, label_1_size, emb_dim, vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 ninp = vocab_size, \n",
    "                 linps = [label_0_size, label_1_size], \n",
    "                 emb_dim = emb_dim, \n",
    "                 emb_lab = 20,\n",
    "                 nhid = hidden_dim, \n",
    "                 nout = vocab_size, \n",
    "                 nlayers = layers, \n",
    "                 dropout = dropout, \n",
    "                 vectors = vectors,\n",
    "                 pretrained = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ninp = ninp\n",
    "        self.linp_offsets = [0] + linps\n",
    "        self.linp = sum(linps)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_lab = emb_lab\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.nlayers = nlayers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.word_embedding = nn.Embedding(ninp, emb_dim)\n",
    "        self.label_embedding = nn.Embedding(self.linp, emb_lab)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim + emb_lab*len(linps), nhid, nlayers, dropout=dropout)\n",
    "        self.rnn.flatten_parameters()\n",
    "        self.decoder = nn.Linear(nhid, nout)\n",
    "\n",
    "        if pretrained:\n",
    "            self.word_embedding.weight.data.copy_(vectors)\n",
    "            self.word_embedding.from_pretrained(GLOVE_EMBEDDING.vectors)\n",
    "            \n",
    "        self.init_weights()    \n",
    "            \n",
    "    def init_weights(self):\n",
    "        initrange = .1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, reviews, labels_list, hidden):\n",
    "        X = self.word_embedding(reviews)\n",
    "\n",
    "        for i, labels in enumerate(labels_list):\n",
    "            labels = self.linp_offsets[i] + labels\n",
    "            L = self.label_embedding(labels)\n",
    "            L = torch.cat([L.unsqueeze(0)]*X.shape[0])\n",
    "            X = torch.cat([X, L], -1)\n",
    "            \n",
    "        X = self.drop(X)\n",
    "        X, hidden = self.rnn(X, hidden)\n",
    "        X = self.drop(X)\n",
    "        \n",
    "        X = self.decoder(X)\n",
    "        return X, hidden\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, criterion):\n",
    "    model.eval()\n",
    "    total_loss_e = 0\n",
    "    total_number_of_words = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_source):\n",
    "            labels_0 = batch.theme.cuda().long() - 1\n",
    "            labels_1 = batch.perspective.cuda().long() - 1\n",
    "            labels = (labels_0, labels_1)\n",
    "            batch = batch.review_text.cuda().long()\n",
    "            hidden = None\n",
    "            if batch.shape[0] > 3:\n",
    "                data, targets = batch[1:-1,:], batch[2:,:]\n",
    "                \n",
    "                output, hidden = model(data, labels, hidden)\n",
    "                output_flat = output.contiguous().view(-1, vocab_size)\n",
    "                target_flat = targets.contiguous().view(-1)\n",
    "                #print(data.shape, output_flat.shape, target_flat.shape)\n",
    "                #0/0\n",
    "                batch_loss = criterion(output_flat, target_flat).detach().item()\n",
    "\n",
    "                number_of_words = data.shape[0] * data.shape[1]\n",
    "                total_loss_e += batch_loss * number_of_words\n",
    "                total_number_of_words += number_of_words\n",
    "\n",
    "                #hidden = \n",
    "                repackage_hidden(hidden)\n",
    "                #print(total_loss_e / total_number_of_words)\n",
    "            \n",
    "    return total_loss_e / total_number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ep0, epN, train_iter, dev_iter, optimizer, criterion, \n",
    "          max_grad_norm, model_name, best_ppl = float('inf')):\n",
    "    \n",
    "    best_ppl = best_ppl\n",
    "    \n",
    "    len_train_iter = len(train_iter)\n",
    "    for epoch in range(ep0, epN):\n",
    "        model.train()\n",
    "        total_loss_e = 0\n",
    "        total_number_of_words = 0 \n",
    "        \n",
    "        for i, batch in enumerate(train_iter):\n",
    "\n",
    "            labels_0 = batch.theme.cuda().long() - 1\n",
    "            labels_1 = batch.perspective.cuda().long() - 1\n",
    "            labels = (labels_0, labels_1)\n",
    "            batch = batch.review_text.cuda().long()\n",
    "            hidden = None\n",
    "            if batch.shape[0] > 3:\n",
    "                data, targets = batch[1:-1,:], batch[2:,:]\n",
    "                \n",
    "                \n",
    "                output, hidden = model(data, labels, hidden)\n",
    "                #hidden = \n",
    "                repackage_hidden(hidden)\n",
    "\n",
    "                output_flat = output.contiguous().view(-1, vocab_size)\n",
    "                target_flat = targets.contiguous().view(-1)\n",
    "                #print(data.shape, output_flat.shape, target_flat.shape)\n",
    "                #0/0\n",
    "                batch_loss = criterion(output_flat, target_flat)\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                number_of_words = data.shape[0] * data.shape[1]\n",
    "                total_loss_e += batch_loss.detach().item() * number_of_words\n",
    "                total_number_of_words += number_of_words\n",
    "            \n",
    "                \n",
    "                if i % 500 == 0:\n",
    "                    cur_loss = batch_loss.detach().item() #np.mean(total_loss_e)#\n",
    "                    tr_ppl_print = np.exp(cur_loss)\n",
    "                    print(\"| epoch {:3d} | batch {} / {} | train_loss {} | train_ppl {}\".format(\n",
    "                            epoch, i, len_train_iter, \n",
    "                            np.round(cur_loss, 3), np.round(tr_ppl_print, 3)))\n",
    "\n",
    "                #gc.collect()\n",
    "                #torch.cuda.empty_cache()\n",
    "                \n",
    "                if i % 4999 == 1: #len_train_iter - 1:\n",
    "                    cur_loss = batch_loss.detach().item() #np.mean(total_loss_e) #\n",
    "                    tr_ppl_print = np.exp(cur_loss)\n",
    "                    gc.collect()\n",
    "                    #torch.cuda.empty_cache()\n",
    "                    val_loss_eval = evaluate(model, dev_iter, criterion)\n",
    "                    val_ppl_print = np.exp(val_loss_eval)\n",
    "                    \n",
    "                    template = \"| epoch {:3d} | batch {} / {} | train_loss {} | train_ppl {} | val_loss {} | val_ppl {}\"\n",
    "                    print(template.format(\n",
    "                            epoch, i, len_train_iter, \n",
    "                            np.round(cur_loss, 3), np.round(tr_ppl_print, 3), \n",
    "                            np.round(val_loss_eval, 3), np.round(val_ppl_print, 3)))\n",
    "\n",
    "                    if val_ppl_print < best_ppl :\n",
    "                        print('old best ppl {} new best ppl {}'.format(best_ppl, val_ppl_print))\n",
    "                        best_ppl = val_ppl_print\n",
    "                        best_model_name = '{}{}.model'.format(model_name, best_ppl)\n",
    "                        print('save model...', best_model_name)\n",
    "                        with open(best_model_name, 'wb') as file:\n",
    "                            torch.save(model, file) \n",
    "\n",
    "                    gc.collect()\n",
    "                    model.train()\n",
    "                    \n",
    "                if i == 40000: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./baseline_2c/best_model_81.86675819854015.model', 'rb') as file:\n",
    "    model = torch.load(file) #BaseModel().cuda()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('best_model_name', 'rb') as file:\n",
    "#    model = torch.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = BaseModel(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (word_embedding): Embedding(12304, 300)\n",
       "  (label_embedding): Embedding(7, 20)\n",
       "  (rnn): LSTM(340, 1024, num_layers=2, dropout=0.5)\n",
       "  (decoder): Linear(in_features=1024, out_features=12304, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement PPL\n",
    "#learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean',\n",
    "                       ignore_index=train_dataset.fields[\"review_text\"].vocab.stoi['<pad>']).cuda()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #, betas=(.999, .9999))\n",
    "\n",
    "learning_rate = 0.00001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(.99999, .99999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | batch 500 / 35138 | train_loss 4.498 | train_ppl 89.805\n",
      "| epoch   1 | batch 1000 / 35138 | train_loss 3.521 | train_ppl 33.81\n",
      "| epoch   1 | batch 1500 / 35138 | train_loss 2.979 | train_ppl 19.669\n",
      "| epoch   1 | batch 2000 / 35138 | train_loss 3.449 | train_ppl 31.456\n",
      "| epoch   1 | batch 2500 / 35138 | train_loss 3.769 | train_ppl 43.343\n",
      "| epoch   1 | batch 3000 / 35138 | train_loss 3.751 | train_ppl 42.573\n",
      "| epoch   1 | batch 3500 / 35138 | train_loss 4.294 | train_ppl 73.243\n",
      "| epoch   1 | batch 4000 / 35138 | train_loss 3.65 | train_ppl 38.469\n",
      "| epoch   1 | batch 4500 / 35138 | train_loss 4.533 | train_ppl 93.074\n",
      "| epoch   1 | batch 5000 / 35138 | train_loss 4.62 | train_ppl 101.533\n",
      "| epoch   1 | batch 5000 / 35138 | train_loss 4.62 | train_ppl 101.533 | val_loss 4.868 | val_ppl 130.05\n",
      "| epoch   1 | batch 5500 / 35138 | train_loss 3.846 | train_ppl 46.809\n",
      "| epoch   1 | batch 6000 / 35138 | train_loss 4.178 | train_ppl 65.244\n",
      "| epoch   1 | batch 6500 / 35138 | train_loss 3.868 | train_ppl 47.849\n",
      "| epoch   1 | batch 7000 / 35138 | train_loss 3.842 | train_ppl 46.61\n",
      "| epoch   1 | batch 7500 / 35138 | train_loss 3.705 | train_ppl 40.63\n",
      "| epoch   1 | batch 8000 / 35138 | train_loss 3.326 | train_ppl 27.831\n",
      "| epoch   1 | batch 8500 / 35138 | train_loss 3.829 | train_ppl 46.016\n",
      "| epoch   1 | batch 9000 / 35138 | train_loss 4.203 | train_ppl 66.89\n",
      "| epoch   1 | batch 9500 / 35138 | train_loss 4.035 | train_ppl 56.531\n",
      "| epoch   1 | batch 9999 / 35138 | train_loss 3.768 | train_ppl 43.304 | val_loss 4.328 | val_ppl 75.764\n",
      "old best ppl 81.867 new best ppl 75.7635141129965\n",
      "save model... baseline_2c/best_model_75.7635141129965.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BaseModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | batch 10000 / 35138 | train_loss 3.95 | train_ppl 51.942\n",
      "| epoch   1 | batch 10500 / 35138 | train_loss 4.352 | train_ppl 77.624\n",
      "| epoch   1 | batch 11000 / 35138 | train_loss 4.071 | train_ppl 58.62\n",
      "| epoch   1 | batch 11500 / 35138 | train_loss 3.606 | train_ppl 36.831\n",
      "| epoch   1 | batch 12000 / 35138 | train_loss 3.551 | train_ppl 34.862\n",
      "| epoch   1 | batch 12500 / 35138 | train_loss 3.733 | train_ppl 41.811\n",
      "| epoch   1 | batch 13000 / 35138 | train_loss 3.753 | train_ppl 42.633\n",
      "| epoch   1 | batch 13500 / 35138 | train_loss 4.3 | train_ppl 73.663\n",
      "| epoch   1 | batch 14000 / 35138 | train_loss 4.145 | train_ppl 63.098\n",
      "| epoch   1 | batch 14500 / 35138 | train_loss 4.432 | train_ppl 84.061\n",
      "| epoch   1 | batch 14998 / 35138 | train_loss 4.272 | train_ppl 71.664 | val_loss 4.281 | val_ppl 72.319\n",
      "old best ppl 75.7635141129965 new best ppl 72.31925011074134\n",
      "save model... baseline_2c/best_model_72.31925011074134.model\n",
      "| epoch   1 | batch 15000 / 35138 | train_loss 4.105 | train_ppl 60.627\n",
      "| epoch   1 | batch 15500 / 35138 | train_loss 4.35 | train_ppl 77.509\n",
      "| epoch   1 | batch 16000 / 35138 | train_loss 4.034 | train_ppl 56.501\n",
      "| epoch   1 | batch 16500 / 35138 | train_loss 4.154 | train_ppl 63.693\n",
      "| epoch   1 | batch 17000 / 35138 | train_loss 3.845 | train_ppl 46.737\n",
      "| epoch   1 | batch 17500 / 35138 | train_loss 4.33 | train_ppl 75.967\n",
      "| epoch   1 | batch 18000 / 35138 | train_loss 4.349 | train_ppl 77.4\n",
      "| epoch   1 | batch 18500 / 35138 | train_loss 4.319 | train_ppl 75.144\n",
      "| epoch   1 | batch 19000 / 35138 | train_loss 3.909 | train_ppl 49.862\n",
      "| epoch   1 | batch 19500 / 35138 | train_loss 4.217 | train_ppl 67.835\n",
      "| epoch   1 | batch 19997 / 35138 | train_loss 4.123 | train_ppl 61.773 | val_loss 4.285 | val_ppl 72.632\n",
      "| epoch   1 | batch 20000 / 35138 | train_loss 4.351 | train_ppl 77.56\n",
      "| epoch   1 | batch 20500 / 35138 | train_loss 4.203 | train_ppl 66.875\n",
      "| epoch   1 | batch 21000 / 35138 | train_loss 4.164 | train_ppl 64.306\n",
      "| epoch   1 | batch 21500 / 35138 | train_loss 4.466 | train_ppl 87.019\n",
      "| epoch   1 | batch 22000 / 35138 | train_loss 4.695 | train_ppl 109.414\n",
      "| epoch   1 | batch 22500 / 35138 | train_loss 4.144 | train_ppl 63.047\n",
      "| epoch   1 | batch 23000 / 35138 | train_loss 4.376 | train_ppl 79.549\n",
      "| epoch   1 | batch 23500 / 35138 | train_loss 4.378 | train_ppl 79.649\n",
      "| epoch   1 | batch 24000 / 35138 | train_loss 4.689 | train_ppl 108.75\n",
      "| epoch   1 | batch 24500 / 35138 | train_loss 4.426 | train_ppl 83.571\n",
      "| epoch   1 | batch 24996 / 35138 | train_loss 4.34 | train_ppl 76.71 | val_loss 4.304 | val_ppl 73.96\n",
      "| epoch   1 | batch 25000 / 35138 | train_loss 4.601 | train_ppl 99.623\n",
      "| epoch   1 | batch 25500 / 35138 | train_loss 4.384 | train_ppl 80.138\n",
      "| epoch   1 | batch 26000 / 35138 | train_loss 4.241 | train_ppl 69.467\n",
      "| epoch   1 | batch 26500 / 35138 | train_loss 4.258 | train_ppl 70.648\n",
      "| epoch   1 | batch 27000 / 35138 | train_loss 4.611 | train_ppl 100.617\n",
      "| epoch   1 | batch 27500 / 35138 | train_loss 4.557 | train_ppl 95.262\n",
      "| epoch   1 | batch 28000 / 35138 | train_loss 4.272 | train_ppl 71.686\n",
      "| epoch   1 | batch 28500 / 35138 | train_loss 4.194 | train_ppl 66.313\n",
      "| epoch   1 | batch 29000 / 35138 | train_loss 4.626 | train_ppl 102.061\n",
      "| epoch   1 | batch 29500 / 35138 | train_loss 4.424 | train_ppl 83.413\n",
      "| epoch   1 | batch 29995 / 35138 | train_loss 4.551 | train_ppl 94.765 | val_loss 4.22 | val_ppl 68.019\n",
      "old best ppl 72.31925011074134 new best ppl 68.01870512131813\n",
      "save model... baseline_2c/best_model_68.01870512131813.model\n",
      "| epoch   1 | batch 30000 / 35138 | train_loss 4.604 | train_ppl 99.924\n",
      "| epoch   1 | batch 30500 / 35138 | train_loss 4.731 | train_ppl 113.415\n",
      "| epoch   1 | batch 31000 / 35138 | train_loss 4.8 | train_ppl 121.505\n",
      "| epoch   1 | batch 31500 / 35138 | train_loss 4.358 | train_ppl 78.083\n",
      "| epoch   1 | batch 32000 / 35138 | train_loss 4.623 | train_ppl 101.817\n",
      "| epoch   1 | batch 32500 / 35138 | train_loss 4.791 | train_ppl 120.363\n",
      "| epoch   1 | batch 33000 / 35138 | train_loss 4.897 | train_ppl 133.839\n",
      "| epoch   1 | batch 33500 / 35138 | train_loss 4.548 | train_ppl 94.474\n",
      "| epoch   1 | batch 34000 / 35138 | train_loss 4.691 | train_ppl 108.967\n",
      "| epoch   1 | batch 34500 / 35138 | train_loss 4.419 | train_ppl 83.024\n",
      "| epoch   1 | batch 34994 / 35138 | train_loss 5.117 | train_ppl 166.762 | val_loss 4.199 | val_ppl 66.648\n",
      "old best ppl 68.01870512131813 new best ppl 66.64781221321738\n",
      "save model... baseline_2c/best_model_66.64781221321738.model\n",
      "| epoch   1 | batch 35000 / 35138 | train_loss 4.898 | train_ppl 134.011\n"
     ]
    }
   ],
   "source": [
    "train(model,\n",
    "      ep0 = 1,\n",
    "      epN = 2,\n",
    "      train_iter = train_iter,\n",
    "      dev_iter = val_iter,\n",
    "      optimizer = optimizer,\n",
    "      criterion = criterion,\n",
    "      max_grad_norm = 10,\n",
    "      model_name = 'baseline_2c/best_model_',\n",
    "      best_ppl = 81.867)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | batch 500 / 35138 | train_loss 0.781 | train_ppl 2.184\n",
      "| epoch   0 | batch 1000 / 35138 | train_loss 2.178 | train_ppl 8.825\n",
      "| epoch   0 | batch 1500 / 35138 | train_loss 1.832 | train_ppl 6.243\n",
      "| epoch   0 | batch 2000 / 35138 | train_loss 3.167 | train_ppl 23.728\n",
      "| epoch   0 | batch 2500 / 35138 | train_loss 3.091 | train_ppl 21.994\n",
      "| epoch   0 | batch 3000 / 35138 | train_loss 3.095 | train_ppl 22.083\n",
      "| epoch   0 | batch 3500 / 35138 | train_loss 3.804 | train_ppl 44.894\n",
      "| epoch   0 | batch 4000 / 35138 | train_loss 3.102 | train_ppl 22.244\n",
      "| epoch   0 | batch 4500 / 35138 | train_loss 3.961 | train_ppl 52.502\n",
      "| epoch   0 | batch 5000 / 35138 | train_loss 4.203 | train_ppl 66.883\n",
      "| epoch   0 | batch 5000 / 35138 | train_loss 4.203 | train_ppl 66.883 | val_loss 14.507 | val_ppl 1997565.245\n",
      "old best ppl inf new best ppl 1997565.244942298\n",
      "save model... baseline_2c/best_model_1997565.244942298.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BaseModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | batch 5500 / 35138 | train_loss 3.643 | train_ppl 38.204\n",
      "| epoch   0 | batch 6000 / 35138 | train_loss 4.075 | train_ppl 58.827\n",
      "| epoch   0 | batch 6500 / 35138 | train_loss 3.734 | train_ppl 41.832\n",
      "| epoch   0 | batch 7000 / 35138 | train_loss 3.737 | train_ppl 41.967\n",
      "| epoch   0 | batch 7500 / 35138 | train_loss 3.668 | train_ppl 39.154\n",
      "| epoch   0 | batch 8000 / 35138 | train_loss 3.4 | train_ppl 29.952\n",
      "| epoch   0 | batch 8500 / 35138 | train_loss 3.912 | train_ppl 49.985\n",
      "| epoch   0 | batch 9000 / 35138 | train_loss 4.404 | train_ppl 81.806\n",
      "| epoch   0 | batch 9500 / 35138 | train_loss 4.243 | train_ppl 69.609\n",
      "| epoch   0 | batch 9999 / 35138 | train_loss 3.69 | train_ppl 40.028 | val_loss 10.02 | val_ppl 22477.823\n",
      "old best ppl 1997565.244942298 new best ppl 22477.823373549785\n",
      "save model... baseline_2c/best_model_22477.823373549785.model\n",
      "| epoch   0 | batch 10000 / 35138 | train_loss 4.142 | train_ppl 62.928\n",
      "| epoch   0 | batch 10500 / 35138 | train_loss 4.223 | train_ppl 68.231\n",
      "| epoch   0 | batch 11000 / 35138 | train_loss 4.073 | train_ppl 58.736\n",
      "| epoch   0 | batch 11500 / 35138 | train_loss 3.472 | train_ppl 32.198\n",
      "| epoch   0 | batch 12000 / 35138 | train_loss 3.44 | train_ppl 31.196\n",
      "| epoch   0 | batch 12500 / 35138 | train_loss 3.817 | train_ppl 45.449\n",
      "| epoch   0 | batch 13000 / 35138 | train_loss 3.978 | train_ppl 53.403\n",
      "| epoch   0 | batch 13500 / 35138 | train_loss 4.499 | train_ppl 89.945\n",
      "| epoch   0 | batch 14000 / 35138 | train_loss 4.217 | train_ppl 67.844\n",
      "| epoch   0 | batch 14500 / 35138 | train_loss 4.558 | train_ppl 95.368\n",
      "| epoch   0 | batch 14998 / 35138 | train_loss 4.318 | train_ppl 75.038 | val_loss 6.928 | val_ppl 1020.824\n",
      "old best ppl 22477.823373549785 new best ppl 1020.82425377314\n",
      "save model... baseline_2c/best_model_1020.82425377314.model\n",
      "| epoch   0 | batch 15000 / 35138 | train_loss 4.05 | train_ppl 57.395\n",
      "| epoch   0 | batch 15500 / 35138 | train_loss 4.414 | train_ppl 82.631\n",
      "| epoch   0 | batch 16000 / 35138 | train_loss 4.09 | train_ppl 59.734\n",
      "| epoch   0 | batch 16500 / 35138 | train_loss 4.304 | train_ppl 73.999\n",
      "| epoch   0 | batch 17000 / 35138 | train_loss 3.848 | train_ppl 46.913\n",
      "| epoch   0 | batch 17500 / 35138 | train_loss 4.305 | train_ppl 74.06\n",
      "| epoch   0 | batch 18000 / 35138 | train_loss 4.377 | train_ppl 79.563\n",
      "| epoch   0 | batch 18500 / 35138 | train_loss 4.175 | train_ppl 65.024\n",
      "| epoch   0 | batch 19000 / 35138 | train_loss 3.903 | train_ppl 49.556\n",
      "| epoch   0 | batch 19500 / 35138 | train_loss 4.222 | train_ppl 68.15\n",
      "| epoch   0 | batch 19997 / 35138 | train_loss 4.147 | train_ppl 63.254 | val_loss 5.447 | val_ppl 232.146\n",
      "old best ppl 1020.82425377314 new best ppl 232.14645370131808\n",
      "save model... baseline_2c/best_model_232.14645370131808.model\n",
      "| epoch   0 | batch 20000 / 35138 | train_loss 4.416 | train_ppl 82.729\n",
      "| epoch   0 | batch 20500 / 35138 | train_loss 4.206 | train_ppl 67.083\n",
      "| epoch   0 | batch 21000 / 35138 | train_loss 4.18 | train_ppl 65.344\n",
      "| epoch   0 | batch 21500 / 35138 | train_loss 4.566 | train_ppl 96.2\n",
      "| epoch   0 | batch 22000 / 35138 | train_loss 4.61 | train_ppl 100.476\n",
      "| epoch   0 | batch 22500 / 35138 | train_loss 4.073 | train_ppl 58.752\n",
      "| epoch   0 | batch 23000 / 35138 | train_loss 4.291 | train_ppl 73.021\n",
      "| epoch   0 | batch 23500 / 35138 | train_loss 4.357 | train_ppl 78.039\n",
      "| epoch   0 | batch 24000 / 35138 | train_loss 4.691 | train_ppl 108.935\n",
      "| epoch   0 | batch 24500 / 35138 | train_loss 4.355 | train_ppl 77.858\n",
      "| epoch   0 | batch 24996 / 35138 | train_loss 4.502 | train_ppl 90.166 | val_loss 4.922 | val_ppl 137.208\n",
      "old best ppl 232.14645370131808 new best ppl 137.20839377249175\n",
      "save model... baseline_2c/best_model_137.20839377249175.model\n",
      "| epoch   0 | batch 25000 / 35138 | train_loss 4.54 | train_ppl 93.684\n",
      "| epoch   0 | batch 25500 / 35138 | train_loss 4.247 | train_ppl 69.864\n",
      "| epoch   0 | batch 26000 / 35138 | train_loss 4.136 | train_ppl 62.544\n",
      "| epoch   0 | batch 26500 / 35138 | train_loss 4.207 | train_ppl 67.129\n",
      "| epoch   0 | batch 27000 / 35138 | train_loss 4.523 | train_ppl 92.138\n",
      "| epoch   0 | batch 27500 / 35138 | train_loss 4.523 | train_ppl 92.069\n",
      "| epoch   0 | batch 28000 / 35138 | train_loss 4.18 | train_ppl 65.387\n",
      "| epoch   0 | batch 28500 / 35138 | train_loss 4.124 | train_ppl 61.837\n",
      "| epoch   0 | batch 29000 / 35138 | train_loss 4.548 | train_ppl 94.454\n",
      "| epoch   0 | batch 29500 / 35138 | train_loss 4.29 | train_ppl 72.951\n",
      "| epoch   0 | batch 29995 / 35138 | train_loss 4.582 | train_ppl 97.674 | val_loss 4.669 | val_ppl 106.573\n",
      "old best ppl 137.20839377249175 new best ppl 106.57299154339795\n",
      "save model... baseline_2c/best_model_106.57299154339795.model\n",
      "| epoch   0 | batch 30000 / 35138 | train_loss 4.503 | train_ppl 90.261\n",
      "| epoch   0 | batch 30500 / 35138 | train_loss 4.724 | train_ppl 112.661\n",
      "| epoch   0 | batch 31000 / 35138 | train_loss 4.706 | train_ppl 110.585\n",
      "| epoch   0 | batch 31500 / 35138 | train_loss 4.215 | train_ppl 67.71\n",
      "| epoch   0 | batch 32000 / 35138 | train_loss 4.468 | train_ppl 87.186\n",
      "| epoch   0 | batch 32500 / 35138 | train_loss 4.665 | train_ppl 106.21\n",
      "| epoch   0 | batch 33000 / 35138 | train_loss 4.752 | train_ppl 115.834\n",
      "| epoch   0 | batch 33500 / 35138 | train_loss 4.477 | train_ppl 87.929\n",
      "| epoch   0 | batch 34000 / 35138 | train_loss 4.498 | train_ppl 89.837\n",
      "| epoch   0 | batch 34500 / 35138 | train_loss 4.237 | train_ppl 69.189\n",
      "| epoch   0 | batch 34994 / 35138 | train_loss 4.752 | train_ppl 115.856 | val_loss 4.405 | val_ppl 81.867\n",
      "old best ppl 106.57299154339795 new best ppl 81.86675819854015\n",
      "save model... baseline_2c/best_model_81.86675819854015.model\n",
      "| epoch   0 | batch 35000 / 35138 | train_loss 4.546 | train_ppl 94.212\n",
      "| epoch   1 | batch 500 / 35138 | train_loss 0.594 | train_ppl 1.811\n",
      "| epoch   1 | batch 1000 / 35138 | train_loss 1.532 | train_ppl 4.629\n",
      "| epoch   1 | batch 1500 / 35138 | train_loss 1.514 | train_ppl 4.546\n",
      "| epoch   1 | batch 2000 / 35138 | train_loss 2.742 | train_ppl 15.524\n",
      "| epoch   1 | batch 2500 / 35138 | train_loss 2.668 | train_ppl 14.404\n",
      "| epoch   1 | batch 3000 / 35138 | train_loss 2.691 | train_ppl 14.749\n",
      "| epoch   1 | batch 3500 / 35138 | train_loss 3.044 | train_ppl 20.988\n",
      "| epoch   1 | batch 4000 / 35138 | train_loss 2.591 | train_ppl 13.342\n",
      "| epoch   1 | batch 4500 / 35138 | train_loss 3.431 | train_ppl 30.894\n",
      "| epoch   1 | batch 5000 / 35138 | train_loss 3.487 | train_ppl 32.703\n",
      "| epoch   1 | batch 5000 / 35138 | train_loss 3.487 | train_ppl 32.703 | val_loss 34.64 | val_ppl 1106368763987806.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c50fa11f9ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mmax_grad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'baseline_2c/best_model_'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       best_ppl = float('inf'))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-5bcaa6aa3735>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, ep0, epN, train_iter, dev_iter, optimizer, criterion, max_grad_norm, model_name, best_ppl)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | batch 500 / 35138 | train_loss 4.324 | train_ppl 75.514\n",
      "| epoch   1 | batch 1000 / 35138 | train_loss 3.384 | train_ppl 29.496\n",
      "| epoch   1 | batch 1500 / 35138 | train_loss 2.856 | train_ppl 17.388\n",
      "| epoch   1 | batch 2000 / 35138 | train_loss 3.665 | train_ppl 39.054\n",
      "| epoch   1 | batch 2500 / 35138 | train_loss 3.718 | train_ppl 41.2\n",
      "| epoch   1 | batch 3000 / 35138 | train_loss 3.568 | train_ppl 35.443\n",
      "| epoch   1 | batch 3500 / 35138 | train_loss 4.33 | train_ppl 75.971\n",
      "| epoch   1 | batch 4000 / 35138 | train_loss 3.653 | train_ppl 38.575\n",
      "| epoch   1 | batch 4500 / 35138 | train_loss 4.714 | train_ppl 111.483\n",
      "| epoch   1 | batch 5000 / 35138 | train_loss 4.523 | train_ppl 92.069\n",
      "| epoch   1 | batch 5000 / 35138 | train_loss 4.523 | train_ppl 92.069 | val_loss 4.999 | val_ppl 148.27\n",
      "| epoch   1 | batch 5500 / 35138 | train_loss 3.746 | train_ppl 42.364\n",
      "| epoch   1 | batch 6000 / 35138 | train_loss 4.154 | train_ppl 63.685\n",
      "| epoch   1 | batch 6500 / 35138 | train_loss 3.852 | train_ppl 47.074\n",
      "| epoch   1 | batch 7000 / 35138 | train_loss 4.004 | train_ppl 54.835\n",
      "| epoch   1 | batch 7500 / 35138 | train_loss 3.744 | train_ppl 42.255\n",
      "| epoch   1 | batch 8000 / 35138 | train_loss 3.254 | train_ppl 25.894\n",
      "| epoch   1 | batch 8500 / 35138 | train_loss 3.842 | train_ppl 46.601\n",
      "| epoch   1 | batch 9000 / 35138 | train_loss 4.204 | train_ppl 66.941\n",
      "| epoch   1 | batch 9500 / 35138 | train_loss 4.18 | train_ppl 65.35\n",
      "| epoch   1 | batch 9999 / 35138 | train_loss 3.752 | train_ppl 42.603 | val_loss 4.335 | val_ppl 76.325\n",
      "old best ppl 81.123 new best ppl 76.32543191859847\n",
      "save model... best_model_base_model_ppl_76.32543191859847.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BaseModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 | batch 10000 / 35138 | train_loss 3.968 | train_ppl 52.89\n",
      "| epoch   1 | batch 10500 / 35138 | train_loss 4.172 | train_ppl 64.869\n",
      "| epoch   1 | batch 11000 / 35138 | train_loss 4.073 | train_ppl 58.758\n",
      "| epoch   1 | batch 11500 / 35138 | train_loss 3.799 | train_ppl 44.654\n",
      "| epoch   1 | batch 12000 / 35138 | train_loss 3.576 | train_ppl 35.74\n",
      "| epoch   1 | batch 12500 / 35138 | train_loss 3.696 | train_ppl 40.302\n",
      "| epoch   1 | batch 13000 / 35138 | train_loss 3.823 | train_ppl 45.748\n",
      "| epoch   1 | batch 13500 / 35138 | train_loss 4.23 | train_ppl 68.741\n",
      "| epoch   1 | batch 14000 / 35138 | train_loss 4.263 | train_ppl 70.993\n",
      "| epoch   1 | batch 14500 / 35138 | train_loss 4.444 | train_ppl 85.109\n",
      "| epoch   1 | batch 14998 / 35138 | train_loss 4.306 | train_ppl 74.143 | val_loss 4.289 | val_ppl 72.862\n",
      "old best ppl 76.32543191859847 new best ppl 72.86214824252396\n",
      "save model... best_model_base_model_ppl_72.86214824252396.model\n",
      "| epoch   1 | batch 15000 / 35138 | train_loss 4.047 | train_ppl 57.229\n",
      "| epoch   1 | batch 15500 / 35138 | train_loss 4.234 | train_ppl 68.998\n",
      "| epoch   1 | batch 16000 / 35138 | train_loss 4.02 | train_ppl 55.676\n",
      "| epoch   1 | batch 16500 / 35138 | train_loss 4.18 | train_ppl 65.336\n",
      "| epoch   1 | batch 17000 / 35138 | train_loss 3.902 | train_ppl 49.499\n",
      "| epoch   1 | batch 17500 / 35138 | train_loss 4.34 | train_ppl 76.673\n",
      "| epoch   1 | batch 18000 / 35138 | train_loss 4.401 | train_ppl 81.55\n",
      "| epoch   1 | batch 18500 / 35138 | train_loss 4.385 | train_ppl 80.214\n",
      "| epoch   1 | batch 19000 / 35138 | train_loss 4.025 | train_ppl 55.972\n",
      "| epoch   1 | batch 19500 / 35138 | train_loss 4.233 | train_ppl 68.954\n",
      "| epoch   1 | batch 19997 / 35138 | train_loss 4.167 | train_ppl 64.528 | val_loss 4.278 | val_ppl 72.1\n",
      "old best ppl 72.86214824252396 new best ppl 72.10009645089686\n",
      "save model... best_model_base_model_ppl_72.10009645089686.model\n",
      "| epoch   1 | batch 20000 / 35138 | train_loss 4.443 | train_ppl 85.031\n",
      "| epoch   1 | batch 20500 / 35138 | train_loss 4.178 | train_ppl 65.238\n",
      "| epoch   1 | batch 21000 / 35138 | train_loss 4.173 | train_ppl 64.925\n",
      "| epoch   1 | batch 21500 / 35138 | train_loss 4.45 | train_ppl 85.625\n",
      "| epoch   1 | batch 22000 / 35138 | train_loss 4.705 | train_ppl 110.489\n",
      "| epoch   1 | batch 22500 / 35138 | train_loss 4.146 | train_ppl 63.151\n",
      "| epoch   1 | batch 23000 / 35138 | train_loss 4.372 | train_ppl 79.215\n",
      "| epoch   1 | batch 23500 / 35138 | train_loss 4.338 | train_ppl 76.587\n",
      "| epoch   1 | batch 24000 / 35138 | train_loss 4.734 | train_ppl 113.803\n",
      "| epoch   1 | batch 24500 / 35138 | train_loss 4.488 | train_ppl 88.915\n",
      "| epoch   1 | batch 24996 / 35138 | train_loss 4.477 | train_ppl 87.984 | val_loss 4.276 | val_ppl 71.936\n",
      "old best ppl 72.10009645089686 new best ppl 71.93551652539986\n",
      "save model... best_model_base_model_ppl_71.93551652539986.model\n",
      "| epoch   1 | batch 25000 / 35138 | train_loss 4.68 | train_ppl 107.76\n",
      "| epoch   1 | batch 25500 / 35138 | train_loss 4.288 | train_ppl 72.837\n",
      "| epoch   1 | batch 26000 / 35138 | train_loss 4.213 | train_ppl 67.591\n",
      "| epoch   1 | batch 26500 / 35138 | train_loss 4.397 | train_ppl 81.17\n",
      "| epoch   1 | batch 27000 / 35138 | train_loss 4.619 | train_ppl 101.42\n",
      "| epoch   1 | batch 27500 / 35138 | train_loss 4.565 | train_ppl 96.066\n",
      "| epoch   1 | batch 28000 / 35138 | train_loss 4.352 | train_ppl 77.615\n",
      "| epoch   1 | batch 28500 / 35138 | train_loss 4.229 | train_ppl 68.668\n",
      "| epoch   1 | batch 29000 / 35138 | train_loss 4.655 | train_ppl 105.099\n",
      "| epoch   1 | batch 29500 / 35138 | train_loss 4.504 | train_ppl 90.409\n",
      "| epoch   1 | batch 29995 / 35138 | train_loss 4.589 | train_ppl 98.414 | val_loss 4.238 | val_ppl 69.267\n",
      "old best ppl 71.93551652539986 new best ppl 69.26713513955411\n",
      "save model... best_model_base_model_ppl_69.26713513955411.model\n",
      "| epoch   1 | batch 30000 / 35138 | train_loss 4.591 | train_ppl 98.547\n",
      "| epoch   1 | batch 30500 / 35138 | train_loss 4.805 | train_ppl 122.102\n",
      "| epoch   1 | batch 31000 / 35138 | train_loss 4.848 | train_ppl 127.516\n",
      "| epoch   1 | batch 31500 / 35138 | train_loss 4.431 | train_ppl 84.022\n",
      "| epoch   1 | batch 32000 / 35138 | train_loss 4.615 | train_ppl 100.998\n",
      "| epoch   1 | batch 32500 / 35138 | train_loss 4.826 | train_ppl 124.739\n",
      "| epoch   1 | batch 33000 / 35138 | train_loss 5.008 | train_ppl 149.549\n",
      "| epoch   1 | batch 33500 / 35138 | train_loss 4.614 | train_ppl 100.869\n",
      "| epoch   1 | batch 34000 / 35138 | train_loss 4.729 | train_ppl 113.18\n",
      "| epoch   1 | batch 34500 / 35138 | train_loss 4.488 | train_ppl 88.931\n",
      "| epoch   1 | batch 34994 / 35138 | train_loss 5.17 | train_ppl 175.869 | val_loss 4.213 | val_ppl 67.546\n",
      "old best ppl 69.26713513955411 new best ppl 67.5461793533482\n",
      "save model... best_model_base_model_ppl_67.5461793533482.model\n",
      "| epoch   1 | batch 35000 / 35138 | train_loss 4.92 | train_ppl 136.967\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | batch 500 / 35138 | train_loss 0.815 | train_ppl 2.259\n",
      "| epoch   0 | batch 1000 / 35138 | train_loss 2.182 | train_ppl 8.864\n",
      "| epoch   0 | batch 1500 / 35138 | train_loss 1.843 | train_ppl 6.317\n",
      "| epoch   0 | batch 2000 / 35138 | train_loss 3.241 | train_ppl 25.552\n",
      "| epoch   0 | batch 2500 / 35138 | train_loss 3.152 | train_ppl 23.389\n",
      "| epoch   0 | batch 3000 / 35138 | train_loss 3.182 | train_ppl 24.102\n",
      "| epoch   0 | batch 3500 / 35138 | train_loss 3.82 | train_ppl 45.592\n",
      "| epoch   0 | batch 4000 / 35138 | train_loss 3.089 | train_ppl 21.948\n",
      "| epoch   0 | batch 4500 / 35138 | train_loss 4.03 | train_ppl 56.244\n",
      "| epoch   0 | batch 5000 / 35138 | train_loss 4.202 | train_ppl 66.84\n",
      "| epoch   0 | batch 5000 / 35138 | train_loss 4.202 | train_ppl 66.84 | val_loss 13.078 | val_ppl 478201.324\n",
      "old best ppl inf new best ppl 478201.3236771168\n",
      "save model... best_model_base_model_ppl_478201.3236771168.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BaseModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | batch 5500 / 35138 | train_loss 3.617 | train_ppl 37.213\n",
      "| epoch   0 | batch 6000 / 35138 | train_loss 4.088 | train_ppl 59.595\n",
      "| epoch   0 | batch 6500 / 35138 | train_loss 3.65 | train_ppl 38.475\n",
      "| epoch   0 | batch 7000 / 35138 | train_loss 3.726 | train_ppl 41.518\n",
      "| epoch   0 | batch 7500 / 35138 | train_loss 3.786 | train_ppl 44.078\n",
      "| epoch   0 | batch 8000 / 35138 | train_loss 3.211 | train_ppl 24.793\n",
      "| epoch   0 | batch 8500 / 35138 | train_loss 3.974 | train_ppl 53.184\n",
      "| epoch   0 | batch 9000 / 35138 | train_loss 4.268 | train_ppl 71.381\n",
      "| epoch   0 | batch 9500 / 35138 | train_loss 4.298 | train_ppl 73.516\n",
      "| epoch   0 | batch 9999 / 35138 | train_loss 3.833 | train_ppl 46.184 | val_loss 10.383 | val_ppl 32308.643\n",
      "old best ppl 478201.3236771168 new best ppl 32308.64283447535\n",
      "save model... best_model_base_model_ppl_32308.64283447535.model\n",
      "| epoch   0 | batch 10000 / 35138 | train_loss 4.086 | train_ppl 59.524\n",
      "| epoch   0 | batch 10500 / 35138 | train_loss 4.34 | train_ppl 76.696\n",
      "| epoch   0 | batch 11000 / 35138 | train_loss 3.963 | train_ppl 52.638\n",
      "| epoch   0 | batch 11500 / 35138 | train_loss 3.541 | train_ppl 34.486\n",
      "| epoch   0 | batch 12000 / 35138 | train_loss 3.488 | train_ppl 32.714\n",
      "| epoch   0 | batch 12500 / 35138 | train_loss 3.73 | train_ppl 41.678\n",
      "| epoch   0 | batch 13000 / 35138 | train_loss 4.004 | train_ppl 54.824\n",
      "| epoch   0 | batch 13500 / 35138 | train_loss 4.506 | train_ppl 90.571\n",
      "| epoch   0 | batch 14000 / 35138 | train_loss 4.259 | train_ppl 70.735\n",
      "| epoch   0 | batch 14500 / 35138 | train_loss 4.513 | train_ppl 91.189\n",
      "| epoch   0 | batch 14998 / 35138 | train_loss 4.369 | train_ppl 78.958 | val_loss 7.131 | val_ppl 1250.174\n",
      "old best ppl 32308.64283447535 new best ppl 1250.1738066458024\n",
      "save model... best_model_base_model_ppl_1250.1738066458024.model\n",
      "| epoch   0 | batch 15000 / 35138 | train_loss 4.024 | train_ppl 55.896\n",
      "| epoch   0 | batch 15500 / 35138 | train_loss 4.412 | train_ppl 82.439\n",
      "| epoch   0 | batch 16000 / 35138 | train_loss 4.056 | train_ppl 57.725\n",
      "| epoch   0 | batch 16500 / 35138 | train_loss 4.297 | train_ppl 73.496\n",
      "| epoch   0 | batch 17000 / 35138 | train_loss 3.809 | train_ppl 45.084\n",
      "| epoch   0 | batch 17500 / 35138 | train_loss 4.307 | train_ppl 74.202\n",
      "| epoch   0 | batch 18000 / 35138 | train_loss 4.364 | train_ppl 78.582\n",
      "| epoch   0 | batch 18500 / 35138 | train_loss 4.212 | train_ppl 67.513\n",
      "| epoch   0 | batch 19000 / 35138 | train_loss 3.97 | train_ppl 52.991\n",
      "| epoch   0 | batch 19500 / 35138 | train_loss 4.217 | train_ppl 67.802\n",
      "| epoch   0 | batch 19997 / 35138 | train_loss 4.13 | train_ppl 62.171 | val_loss 5.574 | val_ppl 263.36\n",
      "old best ppl 1250.1738066458024 new best ppl 263.36047129232475\n",
      "save model... best_model_base_model_ppl_263.36047129232475.model\n",
      "| epoch   0 | batch 20000 / 35138 | train_loss 4.456 | train_ppl 86.177\n",
      "| epoch   0 | batch 20500 / 35138 | train_loss 4.166 | train_ppl 64.454\n",
      "| epoch   0 | batch 21000 / 35138 | train_loss 4.106 | train_ppl 60.705\n",
      "| epoch   0 | batch 21500 / 35138 | train_loss 4.539 | train_ppl 93.596\n",
      "| epoch   0 | batch 22000 / 35138 | train_loss 4.646 | train_ppl 104.127\n",
      "| epoch   0 | batch 22500 / 35138 | train_loss 4.081 | train_ppl 59.181\n",
      "| epoch   0 | batch 23000 / 35138 | train_loss 4.321 | train_ppl 75.247\n",
      "| epoch   0 | batch 23500 / 35138 | train_loss 4.331 | train_ppl 75.989\n",
      "| epoch   0 | batch 24000 / 35138 | train_loss 4.747 | train_ppl 115.203\n",
      "| epoch   0 | batch 24500 / 35138 | train_loss 4.294 | train_ppl 73.233\n",
      "| epoch   0 | batch 24996 / 35138 | train_loss 4.471 | train_ppl 87.447 | val_loss 4.959 | val_ppl 142.424\n",
      "old best ppl 263.36047129232475 new best ppl 142.42391505822505\n",
      "save model... best_model_base_model_ppl_142.42391505822505.model\n",
      "| epoch   0 | batch 25000 / 35138 | train_loss 4.618 | train_ppl 101.269\n",
      "| epoch   0 | batch 25500 / 35138 | train_loss 4.371 | train_ppl 79.087\n",
      "| epoch   0 | batch 26000 / 35138 | train_loss 4.163 | train_ppl 64.269\n",
      "| epoch   0 | batch 26500 / 35138 | train_loss 4.206 | train_ppl 67.077\n",
      "| epoch   0 | batch 27000 / 35138 | train_loss 4.605 | train_ppl 99.937\n",
      "| epoch   0 | batch 27500 / 35138 | train_loss 4.53 | train_ppl 92.74\n",
      "| epoch   0 | batch 28000 / 35138 | train_loss 4.175 | train_ppl 65.065\n",
      "| epoch   0 | batch 28500 / 35138 | train_loss 4.162 | train_ppl 64.208\n",
      "| epoch   0 | batch 29000 / 35138 | train_loss 4.511 | train_ppl 90.991\n",
      "| epoch   0 | batch 29500 / 35138 | train_loss 4.239 | train_ppl 69.306\n",
      "| epoch   0 | batch 29995 / 35138 | train_loss 4.572 | train_ppl 96.698 | val_loss 4.654 | val_ppl 104.959\n",
      "old best ppl 142.42391505822505 new best ppl 104.95860287001192\n",
      "save model... best_model_base_model_ppl_104.95860287001192.model\n",
      "| epoch   0 | batch 30000 / 35138 | train_loss 4.47 | train_ppl 87.368\n",
      "| epoch   0 | batch 30500 / 35138 | train_loss 4.735 | train_ppl 113.903\n",
      "| epoch   0 | batch 31000 / 35138 | train_loss 4.755 | train_ppl 116.114\n",
      "| epoch   0 | batch 31500 / 35138 | train_loss 4.227 | train_ppl 68.497\n",
      "| epoch   0 | batch 32000 / 35138 | train_loss 4.482 | train_ppl 88.421\n",
      "| epoch   0 | batch 32500 / 35138 | train_loss 4.692 | train_ppl 109.061\n",
      "| epoch   0 | batch 33000 / 35138 | train_loss 4.758 | train_ppl 116.495\n",
      "| epoch   0 | batch 33500 / 35138 | train_loss 4.507 | train_ppl 90.619\n",
      "| epoch   0 | batch 34000 / 35138 | train_loss 4.533 | train_ppl 93.077\n",
      "| epoch   0 | batch 34500 / 35138 | train_loss 4.242 | train_ppl 69.551\n",
      "| epoch   0 | batch 34994 / 35138 | train_loss 4.757 | train_ppl 116.369 | val_loss 4.396 | val_ppl 81.123\n",
      "old best ppl 104.95860287001192 new best ppl 81.12277368937825\n",
      "save model... best_model_base_model_ppl_81.12277368937825.model\n",
      "| epoch   0 | batch 35000 / 35138 | train_loss 4.573 | train_ppl 96.808\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.212811502530497"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_list, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.206123341756414"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_iter, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.09592700457719"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.206123341756414)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
