{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generators import get_iterator, get_dataset\n",
    "from classifiers import theme_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe \n",
    "GLOVE_EMBEDDING = GloVe(name=\"6B\", dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset, review_text_FIELD, theme_FIELD = get_dataset(vectors = \n",
    "                                                                                       GLOVE_EMBEDDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_iter = get_iterator(train_dataset, batch_size, train=True, shuffle=False, repeat=False)\n",
    "val_iter = get_iterator(val_dataset, batch_size, train=False, shuffle=False, repeat=False)\n",
    "test_iter = get_iterator(test_dataset, batch_size, train=False, shuffle=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_list = list(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> as usual sad true story ... but great acting <eos> | plot\n",
      "<sos> anything with jennifer lawrence is worth your time . <eos> | other\n",
      "<sos> you 'll find them in `` dirty grandpa . <eos> | other\n",
      "<sos> no direction , worthwhile plot , or acting . <eos> | plot\n",
      "<sos> although be warned there are some gory scenes . <eos> | plot\n",
      "<sos> one of the best musical movie for me . <eos> | other\n",
      "<sos> so disappointed in jake gyllenhall for this one . <eos> | other\n",
      "<sos> it takes much more than a balance to walk <eos> | other\n",
      "<sos> that highlights the greatest of a true thriller . <eos> | other\n",
      "<sos> overall , though , it left me cold . <eos> | other\n"
     ]
    }
   ],
   "source": [
    "batch = val_list[600]\n",
    "x = batch.review_text.transpose(1, 0).int()[:10]\n",
    "y = batch.theme.int()\n",
    "\n",
    "for idx in range(x.shape[0]):\n",
    "    #print(x.shape, y.shape)\n",
    "    print(\"{} | {}\".format(' '.join([train_dataset.fields['review_text'].vocab.itos[_] for _ in x[idx]]),\n",
    "         train_dataset.fields['theme'].vocab.itos[y[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_text_FIELD.vocab.vectors.shape, len(review_text_FIELD.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', 'other', 'plot', 'acting', 'effect', 'production']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_FIELD.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30004, 5, 300, torch.Size([30004, 300]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = review_text_FIELD.vocab.vectors.shape[0]\n",
    "label_size = len(theme_FIELD.vocab) - 1\n",
    "emb_dim = review_text_FIELD.vocab.vectors.shape[1]\n",
    "vectors = train_dataset.fields[\"review_text\"].vocab.vectors\n",
    "hidden_dim = 500\n",
    "layers = 2\n",
    "dropout = .2\n",
    "\n",
    "vocab_size, label_size, emb_dim, vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 ninp = vocab_size, \n",
    "                 linp = label_size, \n",
    "                 emb_dim = emb_dim, \n",
    "                 emb_lab = 20,\n",
    "                 nhid = hidden_dim, \n",
    "                 nout = vocab_size, \n",
    "                 nlayers = layers, \n",
    "                 dropout = dropout, \n",
    "                 vectors = vectors,\n",
    "                 pretrained = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ninp = ninp\n",
    "        self.linp = linp\n",
    "        self.emb_dim = emb_dim\n",
    "        self.emb_lab = emb_lab\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.nlayers = nlayers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.word_embedding = nn.Embedding(ninp, emb_dim)\n",
    "        self.label_embedding = nn.Embedding(linp, emb_lab)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim , nhid, nlayers, dropout=dropout) #+ emb_lab\n",
    "        self.rnn.flatten_parameters()\n",
    "        self.decoder = nn.Linear(nhid, nout)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        if pretrained:\n",
    "            self.word_embedding.weight.data.copy_(vectors)\n",
    "            \n",
    "        self.init_weights()    \n",
    "            \n",
    "    def init_weights(self):\n",
    "        initrange = .1\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, reviews, labels, hidden):\n",
    "        R = self.word_embedding(reviews)\n",
    "        #L = self.label_embedding(labels)\n",
    "        #L = torch.cat([L.unsqueeze(0)]*R.shape[0])\n",
    "        #X = torch.cat([R, L], -1)\n",
    "        X = R\n",
    "        \n",
    "        X = self.drop(X)\n",
    "        X, hidden = self.rnn(X, hidden)\n",
    "        X = self.drop(X)\n",
    "        \n",
    "        X = X.view(X.size(0)*X.size(1), X.size(2))\n",
    "        \n",
    "        X = self.decoder(X)\n",
    "        log_probs = self.softmax(X)\n",
    "        return log_probs, hidden\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss_e = 0\n",
    "    total_number_of_words = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_source):\n",
    "            labels = batch.theme.cuda().long() - 1\n",
    "            batch = batch.review_text.cuda().long()\n",
    "            hidden = None\n",
    "            if batch.shape[0] > 2:\n",
    "                data, targets = batch[:-1,:], batch[1:,:]\n",
    "                number_of_words = data.shape[0] * data.shape[1]\n",
    "                \n",
    "                output, hidden = model(data, labels, hidden)\n",
    "                output_flat = output.contiguous().view(-1, vocab_size)\n",
    "\n",
    "                total_loss_e += criterion(output_flat, targets.contiguous().view(-1)).data.float()\n",
    "                total_number_of_words += number_of_words\n",
    "                hidden = repackage_hidden(hidden)\n",
    "            \n",
    "    return total_loss_e.item() / total_number_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, ep0, epN, train_iter, dev_iter, optimizer, criterion, \n",
    "          max_grad_norm, model_name, best_ppl = float('inf')):\n",
    "    \n",
    "    best_ppl = best_ppl\n",
    "    \n",
    "    len_train_iter = len(train_iter)\n",
    "    for epoch in range(ep0, epN):\n",
    "        model.train()\n",
    "        total_loss_e = 0\n",
    "        total_number_of_words = 0 \n",
    "        \n",
    "        for i, batch in enumerate(train_iter):\n",
    "\n",
    "            labels = batch.theme.cuda().long() - 1\n",
    "            batch = batch.review_text.cuda().long()\n",
    "            hidden = None\n",
    "            if batch.shape[0] > 2:\n",
    "                data, targets = batch[:-1,:], batch[1:,:]\n",
    "                \n",
    "                number_of_words = data.shape[0] * data.shape[1]\n",
    "                \n",
    "                output, hidden = model(data, labels, hidden)\n",
    "                hidden = repackage_hidden(hidden)\n",
    "\n",
    "                output_flat = output.contiguous().view(-1, vocab_size)\n",
    "                epoch_loss = criterion(output_flat, targets.contiguous().view(-1))\n",
    "                total_loss_e += epoch_loss.data.float()\n",
    "                total_number_of_words += number_of_words\n",
    "            \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),max_grad_norm)\n",
    "                optimizer.zero_grad()\n",
    "                epoch_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if i % 500 == 0:\n",
    "                    cur_loss = epoch_loss.item() / number_of_words\n",
    "                    tr_ppl_print = np.exp(cur_loss)\n",
    "                    print(\"| epoch {:3d} | batch {} / {} | train_loss {} | train_ppl {}\".format(\n",
    "                            epoch, i, len_train_iter, \n",
    "                            np.round(cur_loss, 3), np.round(tr_ppl_print, 3)))\n",
    "\n",
    "                #gc.collect()\n",
    "                #torch.cuda.empty_cache()\n",
    "                \n",
    "                if i % 2000 == 0:\n",
    "                    cur_loss = total_loss_e.item() / total_number_of_words\n",
    "                    tr_ppl_print = np.exp(cur_loss)\n",
    "                    val_loss_eval = evaluate(dev_iter)\n",
    "                    val_ppl_print = np.exp(val_loss_eval)\n",
    "                    \n",
    "                    template = \"| epoch {:3d} | batch {} / {} | train_loss {} | train_ppl {} | val_loss {} | val_ppl {}\"\n",
    "                    print(template.format(\n",
    "                            epoch, i, len_train_iter, \n",
    "                            np.round(cur_loss, 3), np.round(tr_ppl_print, 3), \n",
    "                            np.round(val_loss_eval, 3), np.round(val_ppl_print, 3)))\n",
    "\n",
    "                    if val_ppl_print < best_ppl :\n",
    "                        print('old best ppl {} new best ppl {}'.format(best_ppl, val_ppl_print))\n",
    "                        best_ppl = val_ppl_print\n",
    "                        best_model_name = 'best_model_{}_{}.model'.format(model_name, best_ppl)\n",
    "                        print('save model...', best_model_name)\n",
    "                        with open(best_model_name, 'wb') as file:\n",
    "                            torch.save(model, file) \n",
    "\n",
    "                    gc.collect()\n",
    "                    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('./best_model_base_model_ppl_250.3359726270283.model', 'rb') as file:\n",
    "#    model = torch.load(file)#BaseModel().cuda()\n",
    "#    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaseModel(pretrained=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (drop): Dropout(p=0.2)\n",
       "  (word_embedding): Embedding(30004, 300)\n",
       "  (label_embedding): Embedding(5, 20)\n",
       "  (rnn): LSTM(300, 500, num_layers=2, dropout=0.2)\n",
       "  (decoder): Linear(in_features=500, out_features=30004, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement PPL\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum', \n",
    "                       ignore_index=train_dataset.fields[\"review_text\"].vocab.stoi['<pad>']).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | batch 500 / 21962 | train_loss 10.051 | train_ppl 23187.812\n",
      "| epoch   0 | batch 1000 / 21962 | train_loss 10.09 | train_ppl 24110.268\n",
      "| epoch   0 | batch 1500 / 21962 | train_loss 10.137 | train_ppl 25267.456\n",
      "| epoch   0 | batch 2000 / 21962 | train_loss 10.142 | train_ppl 25399.287\n",
      "| epoch   0 | batch 2000 / 21962 | train_loss 10.079 | train_ppl 23834.063 | val_loss 10.217 | val_ppl 27351.027\n",
      "old best ppl inf new best ppl 27351.026519136412\n",
      "save model... best_model_base_model_ppl_27351.026519136412.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type BaseModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   0 | batch 2500 / 21962 | train_loss 10.14 | train_ppl 25323.917\n",
      "| epoch   0 | batch 3000 / 21962 | train_loss 10.165 | train_ppl 25967.78\n",
      "| epoch   0 | batch 3500 / 21962 | train_loss 10.153 | train_ppl 25665.246\n",
      "| epoch   0 | batch 4000 / 21962 | train_loss 10.156 | train_ppl 25754.451\n",
      "| epoch   0 | batch 4000 / 21962 | train_loss 10.125 | train_ppl 24946.977 | val_loss 10.206 | val_ppl 27052.439\n",
      "old best ppl 27351.026519136412 new best ppl 27052.43865718559\n",
      "save model... best_model_base_model_ppl_27052.43865718559.model\n",
      "| epoch   0 | batch 4500 / 21962 | train_loss 10.106 | train_ppl 24490.528\n",
      "| epoch   0 | batch 5000 / 21962 | train_loss 10.065 | train_ppl 23515.18\n",
      "| epoch   0 | batch 5500 / 21962 | train_loss 10.099 | train_ppl 24316.985\n",
      "| epoch   0 | batch 6000 / 21962 | train_loss 10.115 | train_ppl 24709.99\n",
      "| epoch   0 | batch 6000 / 21962 | train_loss 10.118 | train_ppl 24776.903 | val_loss 10.205 | val_ppl 27028.928\n",
      "old best ppl 27052.43865718559 new best ppl 27028.92846795789\n",
      "save model... best_model_base_model_ppl_27028.92846795789.model\n",
      "| epoch   0 | batch 6500 / 21962 | train_loss 10.101 | train_ppl 24362.899\n",
      "| epoch   0 | batch 7000 / 21962 | train_loss 10.143 | train_ppl 25420.334\n",
      "| epoch   0 | batch 7500 / 21962 | train_loss 10.128 | train_ppl 25043.019\n",
      "| epoch   0 | batch 8000 / 21962 | train_loss 10.122 | train_ppl 24875.881\n",
      "| epoch   0 | batch 8000 / 21962 | train_loss 10.121 | train_ppl 24858.699 | val_loss 10.205 | val_ppl 27036.937\n",
      "| epoch   0 | batch 8500 / 21962 | train_loss 10.136 | train_ppl 25241.294\n",
      "| epoch   0 | batch 9000 / 21962 | train_loss 10.141 | train_ppl 25367.713\n",
      "| epoch   0 | batch 9500 / 21962 | train_loss 10.12 | train_ppl 24827.167\n",
      "| epoch   0 | batch 10000 / 21962 | train_loss 10.138 | train_ppl 25290.691\n",
      "| epoch   0 | batch 10000 / 21962 | train_loss 10.128 | train_ppl 25041.224 | val_loss 10.205 | val_ppl 27036.332\n",
      "| epoch   0 | batch 10500 / 21962 | train_loss 10.158 | train_ppl 25800.274\n",
      "| epoch   0 | batch 11000 / 21962 | train_loss 10.169 | train_ppl 26087.217\n",
      "| epoch   0 | batch 11500 / 21962 | train_loss 10.15 | train_ppl 25581.227\n",
      "| epoch   0 | batch 12000 / 21962 | train_loss 10.163 | train_ppl 25929.195\n",
      "| epoch   0 | batch 12000 / 21962 | train_loss 10.137 | train_ppl 25254.748 | val_loss 10.203 | val_ppl 26996.202\n",
      "old best ppl 27028.92846795789 new best ppl 26996.202435570984\n",
      "save model... best_model_base_model_ppl_26996.202435570984.model\n",
      "| epoch   0 | batch 12500 / 21962 | train_loss 10.18 | train_ppl 26363.415\n",
      "| epoch   0 | batch 13000 / 21962 | train_loss 10.164 | train_ppl 25959.154\n",
      "| epoch   0 | batch 13500 / 21962 | train_loss 10.187 | train_ppl 26554.822\n",
      "| epoch   0 | batch 14000 / 21962 | train_loss 10.195 | train_ppl 26769.593\n",
      "| epoch   0 | batch 14000 / 21962 | train_loss 10.145 | train_ppl 25470.989 | val_loss 10.201 | val_ppl 26937.149\n",
      "old best ppl 26996.202435570984 new best ppl 26937.148579466124\n",
      "save model... best_model_base_model_ppl_26937.148579466124.model\n",
      "| epoch   0 | batch 14500 / 21962 | train_loss 10.172 | train_ppl 26173.311\n",
      "| epoch   0 | batch 15000 / 21962 | train_loss 10.179 | train_ppl 26347.847\n",
      "| epoch   0 | batch 15500 / 21962 | train_loss 10.184 | train_ppl 26480.259\n",
      "| epoch   0 | batch 16000 / 21962 | train_loss 10.189 | train_ppl 26602.765\n",
      "| epoch   0 | batch 16000 / 21962 | train_loss 10.154 | train_ppl 25683.166 | val_loss 10.197 | val_ppl 26821.121\n",
      "old best ppl 26937.148579466124 new best ppl 26821.121039846592\n",
      "save model... best_model_base_model_ppl_26821.121039846592.model\n",
      "| epoch   0 | batch 16500 / 21962 | train_loss 10.197 | train_ppl 26822.666\n",
      "| epoch   0 | batch 17000 / 21962 | train_loss 10.189 | train_ppl 26619.037\n"
     ]
    }
   ],
   "source": [
    "train(model,\n",
    "      ep0 = 0,\n",
    "      epN = 1,\n",
    "      train_iter = train_iter,\n",
    "      dev_iter = val_iter,\n",
    "      optimizer = optimizer,\n",
    "      criterion = criterion,\n",
    "      max_grad_norm = 10,\n",
    "      model_name = 'base_model_ppl',\n",
    "      best_ppl = float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
